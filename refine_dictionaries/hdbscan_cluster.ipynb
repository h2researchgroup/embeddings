{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade numpy\n",
    "!pip install hdbscan\n",
    "!pip install gensim\n",
    "!pip install umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim # for operations related to word embeddings\n",
    "import hdbscan # clustering\n",
    "import umap # dim reduction\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "\n",
    "\n",
    "import sys; sys.path.insert(0, \"../../../data_management/tools/\") # To load functions from files in data_management/tools\n",
    "from textlist_file import write_list, load_list # For saving and loading text lists to/from file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dictionary file paths:\n",
    "culture_path = \"../../Dictionary Mapping/Dictionaries/Culture.csv\"\n",
    "relational_path = \"../../Dictionary Mapping/Dictionaries/Relational.csv\"\n",
    "demographic_path = \"../../Dictionary Mapping/Dictionaries/Demographic.csv\"\n",
    "\n",
    "# Define model paths\n",
    "wem_path = \"../../../models_storage/word_embeddings_data/word2vec_phrased_filtered_300d_aug14.bin\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load word2vec models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load(wem_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Clean Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final dictionary lengths\n",
      "Culture: 56\n",
      "Relational: 108\n",
      "Demographic: 57\n",
      "\n"
     ]
    }
   ],
   "source": [
    "culture = []\n",
    "relational = []\n",
    "demographic = []\n",
    "\n",
    "model_vocab = set(model.wv.vocab)\n",
    "\n",
    "# Only include entries that are also in the current model\n",
    "for item in load_list(culture_path):\n",
    "    item = item.strip(\"\\n\").replace(\",\", \" \")\n",
    "    if item in model_vocab:\n",
    "        culture.append(item.strip(\"\\n\").replace(\",\", \" \"))\n",
    "\n",
    "for item in load_list(relational_path):\n",
    "    item = item.strip(\"\\n\").replace(\",\", \" \")\n",
    "    if item in model_vocab:\n",
    "        relational.append(item.strip(\"\\n\").replace(\",\", \" \"))\n",
    "\n",
    "for item in load_list(demographic_path):\n",
    "    item = item.strip(\"\\n\").replace(\",\", \" \")\n",
    "    if item in model_vocab:\n",
    "        demographic.append(item.strip(\"\\n\").replace(\",\", \" \"))\n",
    "\n",
    "culture = pd.DataFrame(culture, columns=[\"item\"])\n",
    "relational = pd.DataFrame(relational, columns=[\"item\"])\n",
    "demographic = pd.DataFrame(demographic, columns=[\"item\"])\n",
    "\n",
    "perspectives = [culture, relational, demographic]\n",
    "perspective_names = [\"culture\", \"relational\", \"demographic\"]\n",
    "\n",
    "print(\"\"\"\n",
    "Final dictionary lengths\n",
    "Culture: {}\n",
    "Relational: {}\n",
    "Demographic: {}\n",
    "\"\"\".format(len(culture), len(relational), len(demographic)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item</th>\n",
       "      <th>wem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ambiguity</td>\n",
       "      <td>[-0.040271938, -0.050914586, -0.08996192, 0.07...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ambiguous</td>\n",
       "      <td>[-0.04765881, 0.029175187, 0.14534585, 0.24998...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>appropriate</td>\n",
       "      <td>[0.048566714, -0.003296685, -0.0069924053, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bureaucratization</td>\n",
       "      <td>[0.26042837, -0.099836364, 0.009189517, 0.3966...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ceremonially</td>\n",
       "      <td>[0.112578005, -0.044181783, -0.33859155, 0.091...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                item                                                wem\n",
       "0          ambiguity  [-0.040271938, -0.050914586, -0.08996192, 0.07...\n",
       "1          ambiguous  [-0.04765881, 0.029175187, 0.14534585, 0.24998...\n",
       "2        appropriate  [0.048566714, -0.003296685, -0.0069924053, 0.0...\n",
       "3  bureaucratization  [0.26042837, -0.099836364, 0.009189517, 0.3966...\n",
       "4       ceremonially  [0.112578005, -0.044181783, -0.33859155, 0.091..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look up embeddings from model\n",
    "for perspective in perspectives:\n",
    "    perspective[\"wem\"] = perspective[\"item\"].apply(lambda item: model.wv[item])\n",
    "\n",
    "culture.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Cosine Distances\n",
    "Cosine distance is a more appropriate measure of distance for word embeddings than Euclidian distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "# issue in hdbscan library requires cast to float64: https://github.com/scikit-learn-contrib/hdbscan/issues/71 \n",
    "distance_matrices = [pairwise_distances(np.stack(p['wem']).astype(np.float64), metric='cosine') for p in perspectives]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, distances for combined dictionaries (sanity check on process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = np.vstack([np.stack(p['wem']) for p in perspectives])\n",
    "# redundant calculations, but these are fast\n",
    "combined_distance_matrix = pairwise_distances(combined, metric='cosine').astype(np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Clusters\n",
    "Using HDBScan for unsupervised clustering. This algorithm performs well with noisy data and clusters of varying shapes and densities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed clusters for culture\n",
      "Computed clusters for relational\n",
      "Computed clusters for demographic\n"
     ]
    }
   ],
   "source": [
    "clusterer = hdbscan.HDBSCAN(metric='precomputed')\n",
    "for name, X, df in zip(perspective_names, distance_matrices, perspectives):\n",
    "    clusterer.fit(X)\n",
    "    df['label'] = clusterer.labels_\n",
    "    print(\"Computed clusters for %s\" % name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word counts per cluster\n",
      "culture\n",
      "-1    41\n",
      " 1     8\n",
      " 0     7\n",
      "Name: label, dtype: int64\n",
      "\n",
      "relational\n",
      "-1    81\n",
      " 1    22\n",
      " 0     5\n",
      "Name: label, dtype: int64\n",
      "\n",
      "demographic\n",
      "-1    39\n",
      " 1    10\n",
      " 0     8\n",
      "Name: label, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Word counts per cluster\")\n",
    "for name, df in zip(perspective_names, perspectives):\n",
    "    print(name)\n",
    "    print(df['label'].value_counts())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clusters for the culture dictionary\n",
      "13         diffusion\n",
      "17    homogenization\n",
      "21         imitation\n",
      "24     institutional\n",
      "29       isomorphism\n",
      "36           mimetic\n",
      "38           mimicry\n",
      "Name: item, dtype: object\n",
      "25     institutionalize\n",
      "26    institutionalized\n",
      "30           legitimacy\n",
      "31           legitimate\n",
      "32          legitimated\n",
      "33         legitimating\n",
      "34         legitimation\n",
      "45            normative\n",
      "Name: item, dtype: object\n",
      "0               ambiguity\n",
      "1               ambiguous\n",
      "2             appropriate\n",
      "3       bureaucratization\n",
      "4            ceremonially\n",
      "5                coercion\n",
      "6                coercive\n",
      "7              conformist\n",
      "8              conformity\n",
      "9               decoupled\n",
      "10             decoupling\n",
      "11                diffuse\n",
      "12               diffused\n",
      "14                diverse\n",
      "15              diversity\n",
      "16            homogeneity\n",
      "18             homogenize\n",
      "19            homogenized\n",
      "20                imitate\n",
      "22             innovation\n",
      "23            innovations\n",
      "27             isomorphic\n",
      "28         isomorphically\n",
      "35                mimesis\n",
      "37                  mimic\n",
      "39                  model\n",
      "40                modeled\n",
      "41               modeling\n",
      "42                   myth\n",
      "43                  myths\n",
      "44                   norm\n",
      "46             profession\n",
      "47           professional\n",
      "48    professionalization\n",
      "49          professionals\n",
      "50            professions\n",
      "51        rationalization\n",
      "52            rationalize\n",
      "53             similarity\n",
      "54            structurate\n",
      "55          structuration\n",
      "Name: item, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#TO DO: clean this up\n",
    "i = 0\n",
    "df = perspectives[i]\n",
    "print(\"Clusters for the %s dictionary\" % perspective_names[i])\n",
    "print(df.groupby('label').get_group(0)[\"item\"])\n",
    "print(df.groupby('label').get_group(1)[\"item\"])\n",
    "print(df.groupby('label').get_group(-1)[\"item\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clusters for the relational dictionary\n",
      "1        acquisitions\n",
      "42    diversification\n",
      "46       diversifying\n",
      "84             merger\n",
      "85            mergers\n",
      "Name: item, dtype: object\n",
      "16                  comply\n",
      "21             constraints\n",
      "23                 control\n",
      "27               cooperate\n",
      "28              cooperated\n",
      "31             cooperation\n",
      "38              dependence\n",
      "39            dependencies\n",
      "59               influence\n",
      "60              influenced\n",
      "61              influences\n",
      "62             influencing\n",
      "64         interdependence\n",
      "65       interdependencies\n",
      "70              interlocks\n",
      "71     interorganizational\n",
      "72                 lobbied\n",
      "73                lobbying\n",
      "90                networks\n",
      "94               pressured\n",
      "95               pressures\n",
      "103              sanctions\n",
      "Name: item, dtype: object\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "df = perspectives[i]\n",
    "print(\"Clusters for the %s dictionary\" % perspective_names[i])\n",
    "print(df.groupby('label').get_group(0)[\"item\"])\n",
    "print(df.groupby('label').get_group(1)[\"item\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clusters for the demographic dictionary\n",
      "23    institutionalization\n",
      "24       institutionalized\n",
      "25              legitimacy\n",
      "26              legitimate\n",
      "27             legitimated\n",
      "28             legitimates\n",
      "29            legitimating\n",
      "30            legitimation\n",
      "Name: item, dtype: object\n",
      "6         dynamic\n",
      "8         ecology\n",
      "10      evolution\n",
      "16     generalism\n",
      "17     generalist\n",
      "18    generalists\n",
      "32          niche\n",
      "33         niches\n",
      "50     specialism\n",
      "51     specialist\n",
      "Name: item, dtype: object\n"
     ]
    }
   ],
   "source": [
    "i = 2\n",
    "df = perspectives[i]\n",
    "print(\"Clusters for the %s dictionary\" % perspective_names[i])\n",
    "print(df.groupby('label').get_group(0)[\"item\"])\n",
    "print(df.groupby('label').get_group(1)[\"item\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer = hdbscan.HDBSCAN(metric='precomputed')\n",
    "clusterer.fit(combined_distance_matrix)\n",
    "combined_labels = clusterer.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in perspectives:\n",
    "    dict_len = len(p)\n",
    "    labels, combined_labels = combined_labels[:dict_len], combined_labels[dict_len:]\n",
    "    p[\"combined_label\"] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word counts per cluster\n",
      "culture\n",
      "-1    43\n",
      " 1    13\n",
      "Name: combined_label, dtype: int64\n",
      "\n",
      "relational\n",
      "-1    75\n",
      " 1    20\n",
      " 2     7\n",
      " 0     6\n",
      "Name: combined_label, dtype: int64\n",
      "\n",
      "demographic\n",
      "-1    47\n",
      " 1     9\n",
      " 2     1\n",
      "Name: combined_label, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Word counts per cluster\")\n",
    "for name, df in zip(perspective_names, perspectives):\n",
    "    print(name)\n",
    "    print(df['combined_label'].value_counts())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering on Core Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Demographic</th>\n",
       "      <th>Unnamed: 1</th>\n",
       "      <th>Relational</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Cultural</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>age dependence</td>\n",
       "      <td>NaN</td>\n",
       "      <td>board directors</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ceremonial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>birth rate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>buffer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>coercion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>carrying capacity</td>\n",
       "      <td>NaN</td>\n",
       "      <td>coalition</td>\n",
       "      <td>NaN</td>\n",
       "      <td>coercive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chance survival</td>\n",
       "      <td>NaN</td>\n",
       "      <td>constrain</td>\n",
       "      <td>NaN</td>\n",
       "      <td>conform</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>competition</td>\n",
       "      <td>NaN</td>\n",
       "      <td>constraint</td>\n",
       "      <td>NaN</td>\n",
       "      <td>conformity</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Demographic  Unnamed: 1       Relational  Unnamed: 3    Cultural\n",
       "0     age dependence         NaN  board directors         NaN  ceremonial\n",
       "1         birth rate         NaN           buffer         NaN    coercion\n",
       "2  carrying capacity         NaN        coalition         NaN    coercive\n",
       "3    chance survival         NaN        constrain         NaN     conform\n",
       "4        competition         NaN       constraint         NaN  conformity"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel(\"core_terms.xlsx\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final core list lengths\n",
      "Culture: 31\n",
      "Relational: 31\n",
      "Demographic: 21\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Only include entries that are also in the current model\n",
    "demographic_core = []\n",
    "relational_core = []\n",
    "cultural_core = []\n",
    "for item in df[\"Demographic\"]:\n",
    "    if item in model_vocab:\n",
    "        demographic_core.append(item)\n",
    "for item in df[\"Relational\"]:\n",
    "    if item in model_vocab:\n",
    "        relational_core.append(item)\n",
    "for item in df[\"Cultural\"]:\n",
    "    if item in model_vocab:\n",
    "        cultural_core.append(item)\n",
    "        \n",
    "cultural_core = pd.DataFrame(cultural_core, columns=[\"item\"])\n",
    "relational_core = pd.DataFrame(relational_core, columns=[\"item\"])\n",
    "demographic_core = pd.DataFrame(demographic_core, columns=[\"item\"])\n",
    "\n",
    "perspectives_core = [cultural_core, relational_core, demographic_core]\n",
    "perspective_names_core = [\"cultural_core\", \"relational_core\", \"demographic_core\"]\n",
    "        \n",
    "print(\"\"\"\n",
    "Final core list lengths\n",
    "Culture: {}\n",
    "Relational: {}\n",
    "Demographic: {}\n",
    "\"\"\".format(len(cultural_core), len(relational_core), len(demographic_core)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PandasArray>\n",
       "[         'buffer',       'coalition',       'constrain',      'constraint',\n",
       "         'control',       'cooperate',     'cooperation',           'coopt',\n",
       "      'cooptation',      'dependence',       'dependent', 'diversification',\n",
       "       'diversify',       'dominance',        'exchange',        'external',\n",
       "      'horizontal',       'influence', 'interdependence',  'interdependent',\n",
       "       'interlock',    'interlocking',           'merge',          'merged',\n",
       "          'merger',         'network',         'network',           'power',\n",
       "        'pressure',        'sanction',        'vertical']\n",
       "Length: 31, dtype: object"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relational_core[\"item\"].array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item</th>\n",
       "      <th>wem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ceremonial</td>\n",
       "      <td>[0.017785886, 0.17060874, -0.34285247, 0.24763...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>coercion</td>\n",
       "      <td>[-0.10194491, -0.004256736, -0.10866293, 0.187...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>coercive</td>\n",
       "      <td>[-0.09407789, 0.17365104, -0.0471853, 0.290502...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>conform</td>\n",
       "      <td>[-0.09841556, 0.03873162, -0.0737745, -0.01913...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>conformity</td>\n",
       "      <td>[-0.1256471, 0.09199774, -0.19100225, 0.051814...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         item                                                wem\n",
       "0  ceremonial  [0.017785886, 0.17060874, -0.34285247, 0.24763...\n",
       "1    coercion  [-0.10194491, -0.004256736, -0.10866293, 0.187...\n",
       "2    coercive  [-0.09407789, 0.17365104, -0.0471853, 0.290502...\n",
       "3     conform  [-0.09841556, 0.03873162, -0.0737745, -0.01913...\n",
       "4  conformity  [-0.1256471, 0.09199774, -0.19100225, 0.051814..."
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look up embeddings from model\n",
    "for perspective in perspectives_core:\n",
    "    perspective[\"wem\"] = perspective[\"item\"].apply(lambda item: model.wv[item])\n",
    "\n",
    "cultural_core.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_matrices_core = [pairwise_distances(np.stack(p['wem']).astype(np.float64), metric='cosine') for p in perspectives_core]\n",
    "combined_core = np.vstack([np.stack(p['wem']) for p in perspectives_core])\n",
    "# redundant calculations, but these are fast\n",
    "combined_distance_matrix_core = pairwise_distances(combined_core, metric='cosine').astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed clusters for cultural_core\n",
      "Computed clusters for relational_core\n",
      "Computed clusters for demographic_core\n"
     ]
    }
   ],
   "source": [
    "# min_cluster_size of 3 is the largest value that generates results\n",
    "clusterer = hdbscan.HDBSCAN(metric='precomputed', min_cluster_size=3)\n",
    "for name, X, df in zip(perspective_names_core, distance_matrices_core, perspectives_core):\n",
    "    clusterer.fit(X)\n",
    "    df['label'] = clusterer.labels_\n",
    "    print(\"Computed clusters for %s\" % name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word counts per cluster\n",
      "cultural_core\n",
      "-1    18\n",
      " 1     6\n",
      " 2     4\n",
      " 0     3\n",
      "Name: label, dtype: int64\n",
      "\n",
      "relational_core\n",
      "-1    15\n",
      " 1     8\n",
      " 0     5\n",
      " 2     3\n",
      "Name: label, dtype: int64\n",
      "\n",
      "demographic_core\n",
      "-1    21\n",
      "Name: label, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Word counts per cluster\")\n",
    "for name, df in zip(perspective_names_core, perspectives_core):\n",
    "    print(name)\n",
    "    print(df['label'].value_counts())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clusters for the cultural_core dictionary\n",
      "11      imitation\n",
      "17    isomorphism\n",
      "21        mimetic\n",
      "Name: item, dtype: object\n",
      "14    institutionalize\n",
      "15    institutionalize\n",
      "18          legitimacy\n",
      "19          legitimate\n",
      "20        legitimation\n",
      "27         rationalize\n",
      "Name: item, dtype: object\n",
      "4     conformity\n",
      "22          norm\n",
      "23     normative\n",
      "24         norms\n",
      "Name: item, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df = perspectives_core[0]\n",
    "print(\"Clusters for the %s dictionary\" % perspective_names_core[0])\n",
    "print(df.groupby('label').get_group(0)[\"item\"])\n",
    "print(df.groupby('label').get_group(1)[\"item\"])\n",
    "print(df.groupby('label').get_group(2)[\"item\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clusters for the relational_core dictionary\n",
      "2      constrain\n",
      "4        control\n",
      "8     cooptation\n",
      "17     influence\n",
      "27         power\n",
      "Name: item, dtype: object\n",
      "6         cooperation\n",
      "14           exchange\n",
      "18    interdependence\n",
      "19     interdependent\n",
      "20          interlock\n",
      "21       interlocking\n",
      "25            network\n",
      "26            network\n",
      "Name: item, dtype: object\n",
      "5     cooperate\n",
      "22        merge\n",
      "24       merger\n",
      "Name: item, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df = perspectives_core[1]\n",
    "print(\"Clusters for the %s dictionary\" % perspective_names_core[1])\n",
    "print(df.groupby('label').get_group(0)[\"item\"])\n",
    "print(df.groupby('label').get_group(1)[\"item\"])\n",
    "print(df.groupby('label').get_group(2)[\"item\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer = hdbscan.HDBSCAN(metric='precomputed')\n",
    "clusterer.fit(combined_distance_matrix_core)\n",
    "combined_labels_core = clusterer.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in perspectives_core:\n",
    "    dict_len = len(p)\n",
    "    labels, combined_labels = combined_labels_core[:dict_len], combined_labels_core[dict_len:]\n",
    "    p[\"combined_label\"] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Included in single cluster from cultural_core\n",
      "1             coercion\n",
      "2             coercive\n",
      "3              conform\n",
      "4           conformity\n",
      "9            diffusion\n",
      "11           imitation\n",
      "12          innovation\n",
      "13       institutional\n",
      "14    institutionalize\n",
      "15    institutionalize\n",
      "16          isomorphic\n",
      "17         isomorphism\n",
      "18          legitimacy\n",
      "19          legitimate\n",
      "20        legitimation\n",
      "21             mimetic\n",
      "23           normative\n",
      "24               norms\n",
      "27         rationalize\n",
      "28        rationalized\n",
      "Name: item, dtype: object\n",
      "Included in single cluster from relational_core\n",
      "1           coalition\n",
      "2           constrain\n",
      "3          constraint\n",
      "4             control\n",
      "9          dependence\n",
      "11    diversification\n",
      "12          diversify\n",
      "13          dominance\n",
      "14           exchange\n",
      "15           external\n",
      "16         horizontal\n",
      "17          influence\n",
      "18    interdependence\n",
      "19     interdependent\n",
      "20          interlock\n",
      "21       interlocking\n",
      "23             merged\n",
      "24             merger\n",
      "27              power\n",
      "28           pressure\n",
      "Name: item, dtype: object\n",
      "Included in single cluster from demographic_core\n",
      "1          density\n",
      "2       ecological\n",
      "3          ecology\n",
      "4        evolution\n",
      "9          inertia\n",
      "11      legitimacy\n",
      "12      legitimate\n",
      "13    legitimation\n",
      "14           niche\n",
      "15      population\n",
      "16     reliability\n",
      "17       selection\n",
      "18       selection\n",
      "19      specialism\n",
      "20      specialist\n",
      "Name: item, dtype: object\n"
     ]
    }
   ],
   "source": [
    "for p, name in zip(perspectives_core, perspective_names_core):\n",
    "    print(\"Included in single cluster from %s\" % name)\n",
    "    print(p.groupby('combined_label').get_group(0)[\"item\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction\n",
    "From hdbscan documentation: \"In general HDBSCAN can do well on up to around 50 or 100 dimensional data, but performance can see significant decreases beyond that.\" \n",
    "Our word vectors contain 300 features, 6-7 times more features than dictionary entries for each perspective. With full vectors, HDBScan classifies everything as noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
