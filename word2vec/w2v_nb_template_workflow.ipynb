{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a18b6aea",
   "metadata": {},
   "source": [
    "## Creating Word Embedding Models for 4 decades\n",
    "\n",
    "Computational Literature Review\n",
    "\n",
    "Creator: Jaren Haber, PhD, Nancy Xu\n",
    "\n",
    "Date created: February 15, 2022\n",
    "\n",
    "Date last modified: November 11, 2022\n",
    "\n",
    "This notebook preprocesses training texts, and creates word2vec embedding models for 4 decades (1970-1979,1980-1989,1990-1999,2000-2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2c95f1",
   "metadata": {},
   "source": [
    "## Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f58d489f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "import random as rand\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from clean_text import stopwords_make, punctstr_make, unicode_make, apache_tokenize, clean_sentence_apache \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "## load training data -  these files keep stopwords because Longformer and BERT expect such inputs\n",
    "\n",
    "# def open_test_data(path):\n",
    "#     return open(path, 'rb')\n",
    "\n",
    "# with open_test_data('/home/jovyan/work/models_storage/filtered_preprocessed_texts_65365_022621.pkl') as f:\n",
    "#     full = pickle.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374d55b1",
   "metadata": {},
   "source": [
    "Modifies the jstor id to get consistent format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a43196d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_jstor_id(x, col_name, https = False):\n",
    "    '''\n",
    "    modify jstor id to get the link in the form of www.jstor.org/stable/23057056\n",
    "    '''\n",
    "    \n",
    "    good_parts = []\n",
    "    if not https:\n",
    "        for ii in x[col_name]:\n",
    "            try: \n",
    "                good_parts.append(ii.split('http://')[1])\n",
    "            except:\n",
    "                good_parts.append(ii)\n",
    "    else:\n",
    "        for ii in x[col_name]:\n",
    "            try: \n",
    "                good_parts.append(ii.split('https://')[1])\n",
    "            except:\n",
    "                good_parts.append(ii)\n",
    "        \n",
    "    return good_parts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d537752",
   "metadata": {},
   "source": [
    "Merge with the metadata files with the correct publish dates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02a97f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify jstor id's\n",
    "\n",
    "## combine the data for the correct article dates\n",
    "dates = pd.read_csv('./sample/parts-1-3-metadata.csv')\n",
    "date2 = pd.read_csv('./sample/part-4-metadata.csv')\n",
    "date2.id = modify_jstor_id(date2, 'id')\n",
    "dates.id = modify_jstor_id(dates,'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6abe4e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "combo = pd.concat([dates, date2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "299de1ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "root = '/home/jovyan/work/'\n",
    "meta_fp = root + '/dictionary_methods/code/metadata_combined.h5' \n",
    "\n",
    "df_meta = pd.read_hdf(meta_fp)\n",
    "df_meta.reset_index(drop=False, inplace=True) # extract file name from index\n",
    "\n",
    "# For merging purposes, get ID alone from file name, e.g. 'journal-article-10.2307_2065002' -> '10.2307_2065002'\n",
    "df_meta['edited_filename'] = df_meta['file_name'].apply(lambda x: x[16:]) \n",
    "df_meta = df_meta[[\"edited_filename\", \"article_name\", \"jstor_url\", \"abstract\", \"journal_title\", \"given_names\", \"primary_subject\", \"year\", \"type\"]] # keep only relevant columns\n",
    "\n",
    "df_meta['id'] =  modify_jstor_id(df_meta,'jstor_url', True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ff47585",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = df_meta.merge(combo, on = 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35b167dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doi(string):\n",
    "    return string.split('-')[-1][:-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2560b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "full['edited_filename'] = full['file_name'].apply(get_doi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bc39979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>text</th>\n",
       "      <th>edited_filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/vol_b/data/jstor_data/ocr/journal-article-10....</td>\n",
       "      <td>[[research, note, church_membership, netherlan...</td>\n",
       "      <td>10.2307_1387034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/vol_b/data/jstor_data/ocr/journal-article-10....</td>\n",
       "      <td>[[polish, io_oo, sociological_review, issn, co...</td>\n",
       "      <td>10.2307_41274754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/vol_b/data/jstor_data/ocr/journal-article-10....</td>\n",
       "      <td>[[article, jjdlbsj, grapliy, compassionate, eg...</td>\n",
       "      <td>10.2307_24467156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/vol_b/data/jstor_data/ocr/journal-article-10....</td>\n",
       "      <td>[[reply, allison, more, comparing, regression_...</td>\n",
       "      <td>10.2307_2782279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/vol_b/data/jstor_data/ocr/journal-article-10....</td>\n",
       "      <td>[[determinants, spousal, interaction, marital,...</td>\n",
       "      <td>10.2307_351656</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           file_name  \\\n",
       "0  /vol_b/data/jstor_data/ocr/journal-article-10....   \n",
       "1  /vol_b/data/jstor_data/ocr/journal-article-10....   \n",
       "2  /vol_b/data/jstor_data/ocr/journal-article-10....   \n",
       "3  /vol_b/data/jstor_data/ocr/journal-article-10....   \n",
       "4  /vol_b/data/jstor_data/ocr/journal-article-10....   \n",
       "\n",
       "                                                text   edited_filename  \n",
       "0  [[research, note, church_membership, netherlan...   10.2307_1387034  \n",
       "1  [[polish, io_oo, sociological_review, issn, co...  10.2307_41274754  \n",
       "2  [[article, jjdlbsj, grapliy, compassionate, eg...  10.2307_24467156  \n",
       "3  [[reply, allison, more, comparing, regression_...   10.2307_2782279  \n",
       "4  [[determinants, spousal, interaction, marital,...    10.2307_351656  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea0e4853",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>edited_filename</th>\n",
       "      <th>article_name</th>\n",
       "      <th>jstor_url</th>\n",
       "      <th>abstract</th>\n",
       "      <th>journal_title</th>\n",
       "      <th>given_names</th>\n",
       "      <th>primary_subject</th>\n",
       "      <th>year</th>\n",
       "      <th>type</th>\n",
       "      <th>id</th>\n",
       "      <th>...</th>\n",
       "      <th>url</th>\n",
       "      <th>creator</th>\n",
       "      <th>publisher</th>\n",
       "      <th>language</th>\n",
       "      <th>pageStart</th>\n",
       "      <th>pageEnd</th>\n",
       "      <th>placeOfPublication</th>\n",
       "      <th>wordCount</th>\n",
       "      <th>pageCount</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.2307_351312</td>\n",
       "      <td>Sex-Role Congruency and Marital Quality</td>\n",
       "      <td>https://www.jstor.org/stable/351312</td>\n",
       "      <td>Drawing upon a probability sample of 331 milit...</td>\n",
       "      <td>Journal of Marriage and Family</td>\n",
       "      <td>[Sidney, Hyman P., Riv-Ellen, Stephen, Thomas,...</td>\n",
       "      <td>Sociology</td>\n",
       "      <td>1976</td>\n",
       "      <td>research-article</td>\n",
       "      <td>www.jstor.org/stable/351312</td>\n",
       "      <td>...</td>\n",
       "      <td>http://www.jstor.org/stable/351312</td>\n",
       "      <td>Gary Lee Bowen; Dennis K. Orthner</td>\n",
       "      <td>Wiley</td>\n",
       "      <td>eng</td>\n",
       "      <td>223</td>\n",
       "      <td>230</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5495</td>\n",
       "      <td>8</td>\n",
       "      <td>part-2.jsonl.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.2307_1171381</td>\n",
       "      <td>Bosses, Machines, and Democratic Leadership: P...</td>\n",
       "      <td>https://www.jstor.org/stable/1171381</td>\n",
       "      <td>None</td>\n",
       "      <td>Social Science History</td>\n",
       "      <td>[Sidney, Hyman P., Riv-Ellen, Stephen, Thomas,...</td>\n",
       "      <td>Sociology</td>\n",
       "      <td>1986</td>\n",
       "      <td>research-article</td>\n",
       "      <td>www.jstor.org/stable/1171381</td>\n",
       "      <td>...</td>\n",
       "      <td>http://www.jstor.org/stable/1171381</td>\n",
       "      <td>Philip R. Vandermeer</td>\n",
       "      <td>Cambridge University Press</td>\n",
       "      <td>eng</td>\n",
       "      <td>395</td>\n",
       "      <td>428</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12315</td>\n",
       "      <td>34</td>\n",
       "      <td>part-1.jsonl.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.2307_20832283</td>\n",
       "      <td>RECOGNIZING GENDER BIAS, REJECTING FEMINISM: A...</td>\n",
       "      <td>https://www.jstor.org/stable/20832283</td>\n",
       "      <td>This article explores the degree to which cler...</td>\n",
       "      <td>Sociological Focus</td>\n",
       "      <td>[Sidney, Hyman P., Riv-Ellen, Stephen, Thomas,...</td>\n",
       "      <td>Sociology</td>\n",
       "      <td>2006</td>\n",
       "      <td>research-article</td>\n",
       "      <td>www.jstor.org/stable/20832283</td>\n",
       "      <td>...</td>\n",
       "      <td>http://www.jstor.org/stable/20832283</td>\n",
       "      <td>SUSAN R. CODY</td>\n",
       "      <td>Taylor &amp; Francis, Ltd.</td>\n",
       "      <td>eng</td>\n",
       "      <td>37</td>\n",
       "      <td>53</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9241</td>\n",
       "      <td>17</td>\n",
       "      <td>part-1.jsonl.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.2307_2096207</td>\n",
       "      <td>Survival Chances of Newly Founded Business Org...</td>\n",
       "      <td>https://www.jstor.org/stable/2096207</td>\n",
       "      <td>Human capital theory and organizational ecolog...</td>\n",
       "      <td>American Sociological Review</td>\n",
       "      <td>[Sidney, Hyman P., Riv-Ellen, Stephen, Thomas,...</td>\n",
       "      <td>Sociology</td>\n",
       "      <td>1991</td>\n",
       "      <td>research-article</td>\n",
       "      <td>www.jstor.org/stable/2096207</td>\n",
       "      <td>...</td>\n",
       "      <td>http://www.jstor.org/stable/2096207</td>\n",
       "      <td>Josef Brüderl; Peter Preisendörfer; Rolf Ziegler</td>\n",
       "      <td>American Sociological Association</td>\n",
       "      <td>eng</td>\n",
       "      <td>227</td>\n",
       "      <td>242</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10467</td>\n",
       "      <td>16</td>\n",
       "      <td>part-1.jsonl.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.2307_2391724</td>\n",
       "      <td>Dimensions of Organizational Influence and The...</td>\n",
       "      <td>https://www.jstor.org/stable/2391724</td>\n",
       "      <td>In this study participativeness, centralizatio...</td>\n",
       "      <td>Administrative Science Quarterly</td>\n",
       "      <td>[Sidney, Hyman P., Riv-Ellen, Stephen, Thomas,...</td>\n",
       "      <td>Management &amp; Organizational Behavior</td>\n",
       "      <td>1971</td>\n",
       "      <td>research-article</td>\n",
       "      <td>www.jstor.org/stable/2391724</td>\n",
       "      <td>...</td>\n",
       "      <td>http://www.jstor.org/stable/2391724</td>\n",
       "      <td>Johannes M. Pennings</td>\n",
       "      <td>Sage Publications, Inc.</td>\n",
       "      <td>eng</td>\n",
       "      <td>688</td>\n",
       "      <td>699</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6022</td>\n",
       "      <td>12</td>\n",
       "      <td>part-1.jsonl.gz</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    edited_filename                                       article_name  \\\n",
       "0    10.2307_351312            Sex-Role Congruency and Marital Quality   \n",
       "1   10.2307_1171381  Bosses, Machines, and Democratic Leadership: P...   \n",
       "2  10.2307_20832283  RECOGNIZING GENDER BIAS, REJECTING FEMINISM: A...   \n",
       "3   10.2307_2096207  Survival Chances of Newly Founded Business Org...   \n",
       "4   10.2307_2391724  Dimensions of Organizational Influence and The...   \n",
       "\n",
       "                               jstor_url  \\\n",
       "0    https://www.jstor.org/stable/351312   \n",
       "1   https://www.jstor.org/stable/1171381   \n",
       "2  https://www.jstor.org/stable/20832283   \n",
       "3   https://www.jstor.org/stable/2096207   \n",
       "4   https://www.jstor.org/stable/2391724   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  Drawing upon a probability sample of 331 milit...   \n",
       "1                                               None   \n",
       "2  This article explores the degree to which cler...   \n",
       "3  Human capital theory and organizational ecolog...   \n",
       "4  In this study participativeness, centralizatio...   \n",
       "\n",
       "                      journal_title  \\\n",
       "0    Journal of Marriage and Family   \n",
       "1            Social Science History   \n",
       "2                Sociological Focus   \n",
       "3      American Sociological Review   \n",
       "4  Administrative Science Quarterly   \n",
       "\n",
       "                                         given_names  \\\n",
       "0  [Sidney, Hyman P., Riv-Ellen, Stephen, Thomas,...   \n",
       "1  [Sidney, Hyman P., Riv-Ellen, Stephen, Thomas,...   \n",
       "2  [Sidney, Hyman P., Riv-Ellen, Stephen, Thomas,...   \n",
       "3  [Sidney, Hyman P., Riv-Ellen, Stephen, Thomas,...   \n",
       "4  [Sidney, Hyman P., Riv-Ellen, Stephen, Thomas,...   \n",
       "\n",
       "                        primary_subject  year              type  \\\n",
       "0                             Sociology  1976  research-article   \n",
       "1                             Sociology  1986  research-article   \n",
       "2                             Sociology  2006  research-article   \n",
       "3                             Sociology  1991  research-article   \n",
       "4  Management & Organizational Behavior  1971  research-article   \n",
       "\n",
       "                              id  ...                                   url  \\\n",
       "0    www.jstor.org/stable/351312  ...    http://www.jstor.org/stable/351312   \n",
       "1   www.jstor.org/stable/1171381  ...   http://www.jstor.org/stable/1171381   \n",
       "2  www.jstor.org/stable/20832283  ...  http://www.jstor.org/stable/20832283   \n",
       "3   www.jstor.org/stable/2096207  ...   http://www.jstor.org/stable/2096207   \n",
       "4   www.jstor.org/stable/2391724  ...   http://www.jstor.org/stable/2391724   \n",
       "\n",
       "                                            creator  \\\n",
       "0                 Gary Lee Bowen; Dennis K. Orthner   \n",
       "1                              Philip R. Vandermeer   \n",
       "2                                     SUSAN R. CODY   \n",
       "3  Josef Brüderl; Peter Preisendörfer; Rolf Ziegler   \n",
       "4                              Johannes M. Pennings   \n",
       "\n",
       "                           publisher  language pageStart pageEnd  \\\n",
       "0                              Wiley       eng       223     230   \n",
       "1         Cambridge University Press       eng       395     428   \n",
       "2             Taylor & Francis, Ltd.       eng        37      53   \n",
       "3  American Sociological Association       eng       227     242   \n",
       "4            Sage Publications, Inc.       eng       688     699   \n",
       "\n",
       "  placeOfPublication wordCount pageCount             file  \n",
       "0                NaN      5495         8  part-2.jsonl.gz  \n",
       "1                NaN     12315        34  part-1.jsonl.gz  \n",
       "2                NaN      9241        17  part-1.jsonl.gz  \n",
       "3                NaN     10467        16  part-1.jsonl.gz  \n",
       "4                NaN      6022        12  part-1.jsonl.gz  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb59407d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mm_full = full.merge(m, on = 'edited_filename', how = 'left')[['text', 'edited_filename', 'journal_title', 'publicationYear']]\n",
    "mm_full = mm_full[~mm_full['publicationYear'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90cbecb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>edited_filename</th>\n",
       "      <th>journal_title</th>\n",
       "      <th>publicationYear</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[research, note, church_membership, netherlan...</td>\n",
       "      <td>10.2307_1387034</td>\n",
       "      <td>Journal for the Scientific Study of Religion</td>\n",
       "      <td>1990.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[polish, io_oo, sociological_review, issn, co...</td>\n",
       "      <td>10.2307_41274754</td>\n",
       "      <td>Polish Sociological Review</td>\n",
       "      <td>2000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[article, jjdlbsj, grapliy, compassionate, eg...</td>\n",
       "      <td>10.2307_24467156</td>\n",
       "      <td>Ethnography</td>\n",
       "      <td>2014.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[reply, allison, more, comparing, regression_...</td>\n",
       "      <td>10.2307_2782279</td>\n",
       "      <td>American Journal of Sociology</td>\n",
       "      <td>1995.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[determinants, spousal, interaction, marital,...</td>\n",
       "      <td>10.2307_351656</td>\n",
       "      <td>Journal of Marriage and Family</td>\n",
       "      <td>1983.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text   edited_filename  \\\n",
       "0  [[research, note, church_membership, netherlan...   10.2307_1387034   \n",
       "1  [[polish, io_oo, sociological_review, issn, co...  10.2307_41274754   \n",
       "2  [[article, jjdlbsj, grapliy, compassionate, eg...  10.2307_24467156   \n",
       "3  [[reply, allison, more, comparing, regression_...   10.2307_2782279   \n",
       "4  [[determinants, spousal, interaction, marital,...    10.2307_351656   \n",
       "\n",
       "                                  journal_title  publicationYear  \n",
       "0  Journal for the Scientific Study of Religion           1990.0  \n",
       "1                    Polish Sociological Review           2000.0  \n",
       "2                                   Ethnography           2014.0  \n",
       "3                 American Journal of Sociology           1995.0  \n",
       "4                Journal of Marriage and Family           1983.0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mm_full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69421314",
   "metadata": {},
   "source": [
    "## Remove HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaccc3f9",
   "metadata": {},
   "source": [
    "remove html tags in unstructured text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f32037d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "\n",
    "def get_full_text(text):\n",
    "    full_text=[]\n",
    "    for i in text:\n",
    "        joined = list(itertools.chain(*i))\n",
    "        full_text.append(\" \".join(joined))\n",
    "    return full_text\n",
    "\n",
    "mm_full['full_text'] = get_full_text(mm_full['text'])\n",
    "\n",
    "\n",
    "\n",
    "def remove_tags(article):\n",
    "    article = re.sub('<plain_text> <page sequence=\"1\">', '', article)\n",
    "    article = re.sub(r'</page>(\\<.*?\\>)', ' \\n ', article)\n",
    "    # xml tags\n",
    "    article = re.sub(r'<.*?>', '', article)\n",
    "    article = re.sub(r'<body.*\\n\\s*.*\\s*.*>', '', article)\n",
    "    return article\n",
    "\n",
    "mm_full['text_no_tags'] = mm_full['full_text'].apply(remove_tags)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450cc24b",
   "metadata": {},
   "source": [
    "## Remove stop words, punctuations, lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2a3dfa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.7-py3-none-any.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 5.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from nltk) (8.0.1)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from nltk) (4.62.2)\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading regex-2022.9.13-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (769 kB)\n",
      "\u001b[K     |████████████████████████████████| 769 kB 100.7 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: regex, nltk\n",
      "Successfully installed nltk-3.7 regex-2022.9.13\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3da5dc74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4d8f855",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d3aea51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e168265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep dictionaries of English words\n",
    "from nltk.corpus import words # Dictionary of 236K English words from NLTK\n",
    "english_nltk = set(words.words()) # Make callable\n",
    "english_long = set() # Dictionary of 467K English words from https://github.com/dwyl/english-words\n",
    "# fname =  \"english_words.txt\" # Set file path to long english dictionary\n",
    "# with open(fname, \"r\") as f:\n",
    "#     for word in f:\n",
    "#         english_long.add(word.strip())\n",
    "        \n",
    "def stopwords_make(vocab_path_old = \"\", extend_stopwords = False):\n",
    "    \"\"\"Create stopwords list. \n",
    "    If extend_stopwords is True, create larger stopword list by joining sklearn list to NLTK list.\"\"\"\n",
    "                                                     \n",
    "    stop_word_list = list(set(stopwords.words(\"english\"))) # list of english stopwords\n",
    "\n",
    "    # Add dates to stopwords\n",
    "    for i in range(1,13):\n",
    "        stop_word_list.append(datetime.date(2008, i, 1).strftime('%B'))\n",
    "    for i in range(1,13):\n",
    "        stop_word_list.append((datetime.date(2008, i, 1).strftime('%B')).lower())\n",
    "    for i in range(1, 2100):\n",
    "        stop_word_list.append(str(i))\n",
    "\n",
    "    # Add other common stopwords\n",
    "    stop_word_list.append('00') \n",
    "    stop_word_list.extend(['mr', 'mrs', 'sa', 'fax', 'email', 'phone', 'am', 'pm', 'org', 'com', \n",
    "                           'Menu', 'Contact Us', 'Facebook', 'Calendar', 'Lunch', 'Breakfast', \n",
    "                           'facebook', 'FAQs', 'FAQ', 'faq', 'faqs']) # web stopwords\n",
    "    stop_word_list.extend(['el', 'en', 'la', 'los', 'para', 'las', 'san']) # Spanish stopwords\n",
    "    stop_word_list.extend(['angeles', 'diego', 'harlem', 'bronx', 'austin', 'antonio']) # cities with many charter schools\n",
    "\n",
    "    # Add state names & abbreviations (both uppercase and lowercase) to stopwords\n",
    "    states = ['AL', 'AK', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DC', 'DE', 'FL', \n",
    "              'GA', 'HI', 'ID', 'IL', 'IN', 'IA', 'KS', 'KY', 'LA', 'ME', \n",
    "              'MD', 'MA', 'MI', 'MN', 'MS', 'MO', 'MT', 'NE', 'NV', 'NH', \n",
    "              'NJ', 'NM', 'NY', 'NC', 'ND', 'OH', 'OK', 'OR', 'PA', 'RI', \n",
    "              'SC', 'SD', 'TN', 'TX', 'UT', 'VT', 'VA', 'WA', 'WI', 'WV', 'WY', \n",
    "              'Alabama', 'Alaska', 'Arizona', 'Arkansas', 'California', \n",
    "              'Colorado', 'Connecticut', 'District of Columbia', 'Delaware', 'Florida', \n",
    "              'Georgia', 'Hawaii', 'Idaho', 'Illinois', 'Indiana', \n",
    "              'Iowa', 'Kansas', 'Kentucky', 'Louisiana', 'Maine', \n",
    "              'Maryland', 'Massachusetts', 'Michigan', 'Minnesota', 'Mississippi', \n",
    "              'Missouri', 'Montana', 'Nebraska', 'Nevada', 'New Hampshire', \n",
    "              'New Jersey', 'New Mexico', 'New York', 'North Carolina', 'North Dakota', \n",
    "              'Ohio', 'Oklahoma', 'Oregon', 'Pennsylvania', 'Rhode Island', \n",
    "              'South Carolina', 'South Dakota', 'Tennessee', 'Texas', 'Utah', \n",
    "              'Vermont', 'Virginia', 'Washington', 'Wisconsin', 'West Virginia', 'Wyoming' \n",
    "              'carolina', 'columbia', 'dakota', 'hampshire', 'mexico', 'rhode', 'york']\n",
    "    for state in states:\n",
    "        stop_word_list.append(state)\n",
    "    for state in [state.lower() for state in states]:\n",
    "        stop_word_list.append(state)\n",
    "        \n",
    "    # Add even more stop words:\n",
    "    if extend_stopwords == True:\n",
    "        stop_word_list = text.ENGLISH_STOP_WORDS.union(stop_word_list)\n",
    "        \n",
    "    # If path to old vocab not specified, skip last step and return stop word list thus far\n",
    "    if vocab_path_old == \"\":\n",
    "        return stop_word_list\n",
    "\n",
    "    # Add to stopwords useless and hard-to-formalize words/chars from first chunk of previous model vocab (e.g., a3d0, \\fs19)\n",
    "    # First create whitelist of useful terms probably in that list, explicitly exclude from junk words list both these and words with underscores (common phrases)\n",
    "    whitelist = [\"Pre-K\", \"pre-k\", \"pre-K\", \"preK\", \"prek\", \n",
    "                 \"1st\", \"2nd\", \"3rd\", \"4th\", \"5th\", \"6th\", \"7th\", \"8th\", \"9th\", \"10th\", \"11th\", \"12th\", \n",
    "                 \"1st-grade\", \"2nd-grade\", \"3rd-grade\", \"4th-grade\", \"5th-grade\", \"6th-grade\", \n",
    "                 \"7th-grade\", \"8th-grade\", \"9th-grade\", \"10th-grade\", \"11th-grade\", \"12th-grade\", \n",
    "                 \"1st-grader\", \"2nd-grader\", \"3rd-grader\", \"4th-grader\", \"5th-grader\", \"6th-grader\", \n",
    "                 \"7th-grader\", \"8th-grader\", \"9th-grader\", \"10th-grader\", \"11th-grader\", \"12th-grader\", \n",
    "                 \"1stgrade\", \"2ndgrade\", \"3rdgrade\", \"4thgrade\", \"5thgrade\", \"6thgrade\", \n",
    "                 \"7thgrade\", \"8thgrade\", \"9thgrade\", \"10thgrade\", \"11thgrade\", \"12thgrade\", \n",
    "                 \"1stgrader\", \"2ndgrader\", \"3rdgrader\", \"4thgrader\", \"5thgrader\", \"6thgrader\", \n",
    "                 \"7thgrader\", \"8thgrader\", \"9thgrader\", \"10thgrader\", \"11thgrader\", \"12thgrader\"]\n",
    "    with open(vocab_path_old) as f: # Load vocab from previous model\n",
    "        junk_words = f.read().splitlines() \n",
    "    junk_words = [word for word in junk_words[:8511] if ((not \"_\" in word) \n",
    "                                                         and (not any(term in word for term in whitelist)))]\n",
    "    stop_word_list.extend(junk_words)\n",
    "                                                     \n",
    "    return stop_word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a4543a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords_make(vocab_path_old = \"\", extend_stopwords = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8150636e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "# from nltk.corpus import stopwords\n",
    "# stop_words = stopwords.words('english')\n",
    "# stop_words.extend(['one', 'two', 'three', 'amp', 'may', 'can', 'new', 'also', 'and'])\n",
    "\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "def word_process(tt):\n",
    "    \"\"\"\n",
    "    helper function to lower text, remove stop words, numbers, and empty strings \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    tt = tt.lower()\n",
    "    \n",
    "    punc = '''!()-[]{};:'\"\\, <>./?@#$%^&*_~=\\n'''\n",
    "    # Removing punctuations in string \n",
    "    # Using loop + punctuation string \n",
    "\n",
    "\n",
    "    for ele in tt:  \n",
    "        if ele in punc:  \n",
    "            tt = tt.replace(ele, \" \")  \n",
    "\n",
    "    # read tokens\n",
    "    tokens = tt.split()\n",
    "    lst = [token.translate(punc).lower() for token in tokens ]\n",
    "    \n",
    "    #remove stop words\n",
    "    filtered = []\n",
    "    for i in lst:\n",
    "        if i not in stop_words:\n",
    "            filtered.append(i)\n",
    "    \n",
    "    # removing singular numbers and singular letters\n",
    "    pattern = '[0-9]'\n",
    "    filtered = [re.sub(pattern, '', i) for i in filtered] \n",
    "    new = []\n",
    "    for inp in filtered:\n",
    "        new.append(' '.join( [w for w in inp.split() if len(w)>1] ))\n",
    "        \n",
    "    # filter out empty strings \n",
    "    new = [i for i in new if i] \n",
    "\n",
    "    dt = [d.split() for d in new]\n",
    "    \n",
    "    return dt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fd27da8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4f8f319f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63038/63038 [2:36:12<00:00,  6.73it/s]   \n"
     ]
    }
   ],
   "source": [
    "mm_full['processed'] =  mm_full['text_no_tags'].progress_apply(word_process)\n",
    "\n",
    "\n",
    "mm_full['processed'] = [sum(i, []) for i in mm_full['processed']]\n",
    "\n",
    "combo_train_df = mm_full"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7633f5",
   "metadata": {},
   "source": [
    "## Split datasets into decades & save"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2ab5e4",
   "metadata": {},
   "source": [
    "split the dataset into 4 decades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7b94aa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_decade = combo_train_df[combo_train_df['publicationYear'] <= 1979]\n",
    "second_decade = combo_train_df[(combo_train_df['publicationYear'] >= 1980) & (combo_train_df['publicationYear'] <= 1989) ]\n",
    "\n",
    "third_decade = combo_train_df[(combo_train_df['publicationYear'] >= 1990) & (combo_train_df['publicationYear'] <= 1999) ]\n",
    "\n",
    "\n",
    "fourth_decade = combo_train_df[(combo_train_df['publicationYear'] >= 2000) & (combo_train_df['publicationYear'] <= 2015) ]\n",
    "\n",
    "\n",
    "first_decade.to_csv('first_decade.csv')\n",
    "second_decade.to_csv('second_decade.csv')\n",
    "third_decade.to_csv('third_decade.csv')\n",
    "fourth_decade.to_csv('fourth_decade.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4733a8d",
   "metadata": {},
   "source": [
    "## Preprocess text for each decade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c667a0",
   "metadata": {},
   "source": [
    "### Remove surnames, use enchant library to filter out non-English words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4df91be9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/work/nancyxu'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7d03d2e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "first_decade = pd.read_csv('first_decade.csv')\n",
    "second_decade = pd.read_csv('second_decade.csv')\n",
    "third_decade = pd.read_csv('third_decade.csv')\n",
    "fourth_decade = pd.read_csv('fourth_decade.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "55e167ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1cbc55a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove surnames\n",
    "\n",
    "my_file = open(\"surnames.txt\", \"r\")\n",
    "data = my_file.read()\n",
    "surnames = data.split(\",\")\n",
    "surnames = [i.replace(\"'\", '').strip() for i in surnames]\n",
    "my_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "91939635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyenchant\n",
      "  Downloading pyenchant-3.2.2-py3-none-any.whl (55 kB)\n",
      "\u001b[K     |████████████████████████████████| 55 kB 2.9 MB/s eta 0:00:011\n",
      "\u001b[?25hInstalling collected packages: pyenchant\n",
      "Successfully installed pyenchant-3.2.2\n"
     ]
    }
   ],
   "source": [
    "! pip install pyenchant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6f6a368a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import enchant\n",
    "valid_d = enchant.Dict(\"en_US\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bcbfbff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_for_decade(decade_df, decade):\n",
    "    \"\"\"\n",
    "    remove surnames, filter by enchant dictionary, and filter by word length\n",
    "    \"\"\"\n",
    "    \n",
    "    p = [ast.literal_eval(i) for i in decade_df['processed']]\n",
    "    \n",
    "    processed_again = []\n",
    "\n",
    "    for i in tqdm(p):\n",
    "        k = [j for j in i if j not in surnames]\n",
    "        processed_again.append(k)\n",
    "        \n",
    "    processed_twice = []\n",
    "    for i in tqdm(processed_again):\n",
    "        k = [el for el in i if el.isalpha() and valid_d.check(el)]\n",
    "        processed_twice.append(k)\n",
    "    \n",
    "#     for k in processed_twice:\n",
    "#         processed_again2.append([i for i in k if len(i)>2])\n",
    "    \n",
    "    with open('processed_corp_enchant_' + decade +'.pkl', 'wb') as f:\n",
    "        pickle.dump(processed_twice, f)\n",
    "    \n",
    "    return processed_twice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238b95ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('processed_corp_enchant.pkl', 'rb') as f:\n",
    "    processed_first_decade = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fbca4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('processed_corp_enchant.pkl', 'rb') as f:\n",
    "    processed_second_decade = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342e09f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('processed_corp_enchant.pkl', 'rb') as f:\n",
    "    processed_second_decade = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "78f46e1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6677/6677 [2:13:59<00:00,  1.20s/it]  \n",
      "100%|██████████| 6677/6677 [02:37<00:00, 42.34it/s]\n"
     ]
    }
   ],
   "source": [
    "processed_first_decade = process_for_decade(first_decade, 'first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "341de3d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12222/12222 [4:39:49<00:00,  1.37s/it]  \n",
      "100%|██████████| 12222/12222 [05:37<00:00, 36.26it/s]\n"
     ]
    }
   ],
   "source": [
    "processed_second_decade = process_for_decade(second_decade, 'second')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fbc134",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 9546/15832 [4:28:52<3:42:08,  2.12s/it] "
     ]
    }
   ],
   "source": [
    "processed_third_decade = process_for_decade(third_decade, 'third')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e6a8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_fourth_decade = process_for_decade(fourth_decade, 'fourth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133484f1",
   "metadata": {},
   "source": [
    "## Train gensim phrased word2vec models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3676eae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ecfdafca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acf4daa",
   "metadata": {},
   "source": [
    "build bigrams to create phased w2v models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2e06803c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "def build_phrases(sentences):\n",
    "    phrases = Phrases(sentences, min_count = 5, threshold = 7, progress_per = 1000)\n",
    "    return Phraser(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a89e2b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram1 = build_phrases(processed_first_decade)\n",
    "# bigram2 = build_phrases(processed_second_decade)\n",
    "# bigram3 = build_phrases(processed_third_decade)\n",
    "# bigram4 = build_phrases(processed_fourth_decade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "63592e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6677/6677 [00:20<00:00, 323.47it/s]\n"
     ]
    }
   ],
   "source": [
    "processed_bigrams1 = [bigram1[i] for i in tqdm(processed_first_decade)]\n",
    "# processed_bigrams2 = [bigram2[i] for i in processed_second_decade]\n",
    "# processed_bigrams3 = [bigram3[i] for i in processed_third_decade]\n",
    "# processed_bigrams4 = [bigram4[i] for i in processed_fourth_decade]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0bd5cb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram2 = build_phrases(processed_second_decade)\n",
    "bigram3 = build_phrases(processed_third_decade)\n",
    "bigram4 = build_phrases(processed_fourth_decade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a067f610",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_bigrams2 = [bigram2[i] for i in processed_second_decade]\n",
    "processed_bigrams3 = [bigram3[i] for i in processed_third_decade]\n",
    "processed_bigrams4 = [bigram4[i] for i in processed_fourth_decade]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e156dd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_bigrams_final1 = []\n",
    "processed_bigrams_final2 = []\n",
    "processed_bigrams_final3 = []\n",
    "processed_bigrams_final4 = []\n",
    "\n",
    "## strip punctuations\n",
    "for k in processed_bigrams1:\n",
    "    processed_bigrams_final1.append([i.strip('!\"“#$%&\\'()*+,-./:;<=>?@[\\\\]^`{|}~…') for i in k if len(i)>2])\n",
    "\n",
    "for k in processed_bigrams2:\n",
    "    processed_bigrams_final2.append([i.strip('!\"“#$%&\\'()*+,-./:;<=>?@[\\\\]^`{|}~…') for i in k if len(i)>2])\n",
    "\n",
    "for k in processed_bigrams3:\n",
    "    processed_bigrams_final3.append([i.strip('!\"“#$%&\\'()*+,-./:;<=>?@[\\\\]^`{|}~…') for i in k if len(i)>2])\n",
    "\n",
    "for k in processed_bigrams4:\n",
    "    processed_bigrams_final4.append([i.strip('!\"“#$%&\\'()*+,-./:;<=>?@[\\\\]^`{|}~…') for i in k if len(i)>2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "251bffc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from sklearn import utils\n",
    "cores = multiprocessing.cpu_count()\n",
    "import gensim\n",
    "from gensim.test.utils import get_tmpfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bbe877",
   "metadata": {},
   "source": [
    "create one model for each decade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9e62c2a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved!\n"
     ]
    }
   ],
   "source": [
    "model_decade_1 = gensim.models.Word2Vec(processed_bigrams_final1, vector_size = 300, window = 10,\n",
    "                                       min_count = 5, sg = 1, alpha = 0.05, epochs = 50, \n",
    "                                       batch_words = 10000, workers = cores, seed = 0, negative = 5,ns_exponent = 0.75)\n",
    "\n",
    "\n",
    "\n",
    "fname = \"word2vec_phrased_filtered_enchant_300d_1970_1979_2022_oct30.bin\"\n",
    "model_decade_1.save(fname)\n",
    "print(\"Model Saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4ee531ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved!\n"
     ]
    }
   ],
   "source": [
    "model_decade_2 = gensim.models.Word2Vec(processed_bigrams_final2, vector_size = 300, window = 10,\n",
    "                                       min_count = 5, sg = 1, alpha = 0.05, epochs = 50, \n",
    "                                       batch_words = 10000, workers = cores, seed = 0, negative = 5,ns_exponent = 0.75)\n",
    "\n",
    "\n",
    "\n",
    "fname = \"word2vec_phrased_filtered_enchant_300d_1980_1989_2022_oct30.bin\"\n",
    "model_decade_2.save(fname)\n",
    "print(\"Model Saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e9490760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved!\n"
     ]
    }
   ],
   "source": [
    "model_decade_3 = gensim.models.Word2Vec(processed_bigrams_final3, vector_size = 300, window = 10,\n",
    "                                       min_count = 5, sg = 1, alpha = 0.05, epochs = 50, \n",
    "                                       batch_words = 10000, workers = cores, seed = 0, negative = 5,ns_exponent = 0.75)\n",
    "\n",
    "\n",
    "\n",
    "fname = \"word2vec_phrased_filtered_enchant_300d_1990_1999_2022_oct30.bin\"\n",
    "model_decade_3.save(fname)\n",
    "print(\"Model Saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e3f1d839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved!\n"
     ]
    }
   ],
   "source": [
    "model_decade_4 = gensim.models.Word2Vec(processed_bigrams_final4, vector_size = 300, window = 10,\n",
    "                                       min_count = 5, sg = 1, alpha = 0.05, epochs = 50, \n",
    "                                       batch_words = 10000, workers = cores, seed = 0, negative = 5,ns_exponent = 0.75)\n",
    "\n",
    "\n",
    "\n",
    "fname = \"word2vec_phrased_filtered_enchant_300d_2000_2016_2022_oct30.bin\"\n",
    "model_decade_4.save(fname)\n",
    "print(\"Model Saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7954443a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
