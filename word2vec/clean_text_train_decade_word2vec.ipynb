{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Word Embedding Models for 4 decades\n",
    "\n",
    "Computational Literature Review\n",
    "\n",
    "Creator: Jaren Haber, PhD, Nancy Xu\n",
    "\n",
    "Date created: February 15, 2022\n",
    "\n",
    "Date last modified: April 11, 2022\n",
    "\n",
    "This notebook preprocesses training texts, and creates word2vec embedding models for 4 decades (1970-1979,1980-1989,1990-1999,2000-2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pickle\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "import random as rand\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from clean_text import stopwords_make, punctstr_make, unicode_make, apache_tokenize, clean_sentence_apache \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def open_test_data(path):\n",
    "    return open(path, 'rb')\n",
    "with open_test_data('/home/jovyan/work/tlu_storage/training_cultural_preprocessed_100321.pkl') as f:\n",
    "    cult = pickle.load(f)\n",
    "\n",
    "with open_test_data('/home/jovyan/work/tlu_storage/training_demographic_preprocessed_100321.pkl') as f:\n",
    "    demog = pickle.load(f)\n",
    "\n",
    "with open_test_data('/home/jovyan/work/tlu_storage/training_orgs_preprocessed_100321.pkl') as f:\n",
    "    orgs = pickle.load(f)\n",
    "\n",
    "with open_test_data('/home/jovyan/work/tlu_storage/training_relational_preprocessed_100321.pkl') as f:\n",
    "    rela = pickle.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_jstor_id(x, col_name, https = False):\n",
    "    '''\n",
    "    modify jstor id to get the link in the form of www.jstor.org/stable/23057056\n",
    "    '''\n",
    "    \n",
    "    good_parts = []\n",
    "    if not https:\n",
    "        for ii in x[col_name]:\n",
    "            try: \n",
    "                good_parts.append(ii.split('http://')[1])\n",
    "            except:\n",
    "                good_parts.append(ii)\n",
    "    else:\n",
    "        for ii in x[col_name]:\n",
    "            try: \n",
    "                good_parts.append(ii.split('https://')[1])\n",
    "            except:\n",
    "                good_parts.append(ii)\n",
    "        \n",
    "    return good_parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify jstor id's\n",
    "\n",
    "## combine the data for the correct article dates\n",
    "dates = pd.read_csv('./sample/parts-1-3-metadata.csv')\n",
    "date2 = pd.read_csv('./sample/part-4-metadata.csv')\n",
    "\n",
    "\n",
    "date2.id = modify_jstor_id(date2, 'id')\n",
    "\n",
    "\n",
    "dates.id = modify_jstor_id(dates,'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combo = pd.concat([dates, date2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "root = '/home/jovyan/work/'\n",
    "meta_fp = root + '/dictionary_methods/code/metadata_combined.h5' \n",
    "\n",
    "df_meta = pd.read_hdf(meta_fp)\n",
    "df_meta.reset_index(drop=False, inplace=True) # extract file name from index\n",
    "\n",
    "# For merging purposes, get ID alone from file name, e.g. 'journal-article-10.2307_2065002' -> '10.2307_2065002'\n",
    "df_meta['edited_filename'] = df_meta['file_name'].apply(lambda x: x[16:]) \n",
    "df_meta = df_meta[[\"edited_filename\", \"article_name\", \"jstor_url\", \"abstract\", \"journal_title\", \"given_names\", \"primary_subject\", \"year\", \"type\"]] # keep only relevant columns\n",
    "\n",
    "df_meta['id'] =  modify_jstor_id(df_meta,'jstor_url', True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = df_meta.merge(combo, on = 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_cult = cult.merge(m, on = 'edited_filename', how = 'left')[['text', 'cultural_score', 'edited_filename', 'journal_title', 'publicationYear']]\n",
    "mm_cult = mm_cult[~mm_cult['publicationYear'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_orgs= orgs.merge(m, on = 'edited_filename', how = 'left')[['text', 'orgs_score', 'edited_filename', 'journal_title', 'publicationYear']]\n",
    "mm_orgs = mm_orgs[~mm_orgs['publicationYear'].isna()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_rela= rela.merge(m, on = 'edited_filename', how = 'left')[['text', 'relational_score', 'edited_filename', 'journal_title', 'publicationYear']]\n",
    "mm_rela = mm_rela[~mm_rela['publicationYear'].isna()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_demog= demog.merge(m, on = 'edited_filename', how = 'left')[['text', 'demographic_score', 'edited_filename', 'journal_title', 'publicationYear']]\n",
    "mm_demog= mm_demog[~mm_demog['publicationYear'].isna()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "full_text = []\n",
    "\n",
    "for i in mm_cult['text']:\n",
    "    joined = list(itertools.chain(*i))\n",
    "    full_text.append(\" \".join(joined))\n",
    "\n",
    "full_text_demog = []\n",
    "for i in mm_demog['text']:\n",
    "    joined = list(itertools.chain(*i))\n",
    "    full_text_demog.append(\" \".join(joined))\n",
    "    \n",
    "full_text_orgs = []\n",
    "for j in mm_orgs['text']:\n",
    "    joined = list(itertools.chain(*j))\n",
    "    full_text_orgs.append(\" \".join(joined))\n",
    "\n",
    "full_text_rela = []\n",
    "for j in mm_rela['text']:\n",
    "    joined = list(itertools.chain(*j))\n",
    "    full_text_rela.append(\" \".join(joined))\n",
    "    \n",
    "mm_cult['full_text'] = full_text\n",
    "mm_demog['full_text'] = full_text_demog\n",
    "mm_orgs['full_text'] = full_text_orgs\n",
    "mm_rela['full_text'] = full_text_rela\n",
    "\n",
    "def remove_tags(article):\n",
    "    article = re.sub('<plain_text> <page sequence=\"1\">', '', article)\n",
    "    article = re.sub(r'</page>(\\<.*?\\>)', ' \\n ', article)\n",
    "    # xml tags\n",
    "    article = re.sub(r'<.*?>', '', article)\n",
    "    article = re.sub(r'<body.*\\n\\s*.*\\s*.*>', '', article)\n",
    "    return article\n",
    "\n",
    "tags_removed = [remove_tags(art) for art in mm_cult['full_text']]\n",
    "tags_removed_demog = [remove_tags(art) for art in mm_demog['full_text']]\n",
    "tags_removed_org = [remove_tags(art) for art in mm_orgs['full_text']]\n",
    "tags_removed_rela = [remove_tags(art) for art in mm_rela['full_text']]\n",
    "mm_cult['text_no_tags'] = tags_removed\n",
    "mm_demog['text_no_tags'] = tags_removed_demog\n",
    "mm_orgs['text_no_tags'] = tags_removed_org\n",
    "mm_rela['text_no_tags'] = tags_removed_rela"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove stop words, punctuations, lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep dictionaries of English words\n",
    "from nltk.corpus import words # Dictionary of 236K English words from NLTK\n",
    "english_nltk = set(words.words()) # Make callable\n",
    "english_long = set() # Dictionary of 467K English words from https://github.com/dwyl/english-words\n",
    "# fname =  \"english_words.txt\" # Set file path to long english dictionary\n",
    "# with open(fname, \"r\") as f:\n",
    "#     for word in f:\n",
    "#         english_long.add(word.strip())\n",
    "        \n",
    "def stopwords_make(vocab_path_old = \"\", extend_stopwords = False):\n",
    "    \"\"\"Create stopwords list. \n",
    "    If extend_stopwords is True, create larger stopword list by joining sklearn list to NLTK list.\"\"\"\n",
    "                                                     \n",
    "    stop_word_list = list(set(stopwords.words(\"english\"))) # list of english stopwords\n",
    "\n",
    "    # Add dates to stopwords\n",
    "    for i in range(1,13):\n",
    "        stop_word_list.append(datetime.date(2008, i, 1).strftime('%B'))\n",
    "    for i in range(1,13):\n",
    "        stop_word_list.append((datetime.date(2008, i, 1).strftime('%B')).lower())\n",
    "    for i in range(1, 2100):\n",
    "        stop_word_list.append(str(i))\n",
    "\n",
    "    # Add other common stopwords\n",
    "    stop_word_list.append('00') \n",
    "    stop_word_list.extend(['mr', 'mrs', 'sa', 'fax', 'email', 'phone', 'am', 'pm', 'org', 'com', \n",
    "                           'Menu', 'Contact Us', 'Facebook', 'Calendar', 'Lunch', 'Breakfast', \n",
    "                           'facebook', 'FAQs', 'FAQ', 'faq', 'faqs']) # web stopwords\n",
    "    stop_word_list.extend(['el', 'en', 'la', 'los', 'para', 'las', 'san']) # Spanish stopwords\n",
    "    stop_word_list.extend(['angeles', 'diego', 'harlem', 'bronx', 'austin', 'antonio']) # cities with many charter schools\n",
    "\n",
    "    # Add state names & abbreviations (both uppercase and lowercase) to stopwords\n",
    "    states = ['AL', 'AK', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DC', 'DE', 'FL', \n",
    "              'GA', 'HI', 'ID', 'IL', 'IN', 'IA', 'KS', 'KY', 'LA', 'ME', \n",
    "              'MD', 'MA', 'MI', 'MN', 'MS', 'MO', 'MT', 'NE', 'NV', 'NH', \n",
    "              'NJ', 'NM', 'NY', 'NC', 'ND', 'OH', 'OK', 'OR', 'PA', 'RI', \n",
    "              'SC', 'SD', 'TN', 'TX', 'UT', 'VT', 'VA', 'WA', 'WI', 'WV', 'WY', \n",
    "              'Alabama', 'Alaska', 'Arizona', 'Arkansas', 'California', \n",
    "              'Colorado', 'Connecticut', 'District of Columbia', 'Delaware', 'Florida', \n",
    "              'Georgia', 'Hawaii', 'Idaho', 'Illinois', 'Indiana', \n",
    "              'Iowa', 'Kansas', 'Kentucky', 'Louisiana', 'Maine', \n",
    "              'Maryland', 'Massachusetts', 'Michigan', 'Minnesota', 'Mississippi', \n",
    "              'Missouri', 'Montana', 'Nebraska', 'Nevada', 'New Hampshire', \n",
    "              'New Jersey', 'New Mexico', 'New York', 'North Carolina', 'North Dakota', \n",
    "              'Ohio', 'Oklahoma', 'Oregon', 'Pennsylvania', 'Rhode Island', \n",
    "              'South Carolina', 'South Dakota', 'Tennessee', 'Texas', 'Utah', \n",
    "              'Vermont', 'Virginia', 'Washington', 'Wisconsin', 'West Virginia', 'Wyoming' \n",
    "              'carolina', 'columbia', 'dakota', 'hampshire', 'mexico', 'rhode', 'york']\n",
    "    for state in states:\n",
    "        stop_word_list.append(state)\n",
    "    for state in [state.lower() for state in states]:\n",
    "        stop_word_list.append(state)\n",
    "        \n",
    "    # Add even more stop words:\n",
    "    if extend_stopwords == True:\n",
    "        stop_word_list = text.ENGLISH_STOP_WORDS.union(stop_word_list)\n",
    "        \n",
    "    # If path to old vocab not specified, skip last step and return stop word list thus far\n",
    "    if vocab_path_old == \"\":\n",
    "        return stop_word_list\n",
    "\n",
    "    # Add to stopwords useless and hard-to-formalize words/chars from first chunk of previous model vocab (e.g., a3d0, \\fs19)\n",
    "    # First create whitelist of useful terms probably in that list, explicitly exclude from junk words list both these and words with underscores (common phrases)\n",
    "    whitelist = [\"Pre-K\", \"pre-k\", \"pre-K\", \"preK\", \"prek\", \n",
    "                 \"1st\", \"2nd\", \"3rd\", \"4th\", \"5th\", \"6th\", \"7th\", \"8th\", \"9th\", \"10th\", \"11th\", \"12th\", \n",
    "                 \"1st-grade\", \"2nd-grade\", \"3rd-grade\", \"4th-grade\", \"5th-grade\", \"6th-grade\", \n",
    "                 \"7th-grade\", \"8th-grade\", \"9th-grade\", \"10th-grade\", \"11th-grade\", \"12th-grade\", \n",
    "                 \"1st-grader\", \"2nd-grader\", \"3rd-grader\", \"4th-grader\", \"5th-grader\", \"6th-grader\", \n",
    "                 \"7th-grader\", \"8th-grader\", \"9th-grader\", \"10th-grader\", \"11th-grader\", \"12th-grader\", \n",
    "                 \"1stgrade\", \"2ndgrade\", \"3rdgrade\", \"4thgrade\", \"5thgrade\", \"6thgrade\", \n",
    "                 \"7thgrade\", \"8thgrade\", \"9thgrade\", \"10thgrade\", \"11thgrade\", \"12thgrade\", \n",
    "                 \"1stgrader\", \"2ndgrader\", \"3rdgrader\", \"4thgrader\", \"5thgrader\", \"6thgrader\", \n",
    "                 \"7thgrader\", \"8thgrader\", \"9thgrader\", \"10thgrader\", \"11thgrader\", \"12thgrader\"]\n",
    "    with open(vocab_path_old) as f: # Load vocab from previous model\n",
    "        junk_words = f.read().splitlines() \n",
    "    junk_words = [word for word in junk_words[:8511] if ((not \"_\" in word) \n",
    "                                                         and (not any(term in word for term in whitelist)))]\n",
    "    stop_word_list.extend(junk_words)\n",
    "                                                     \n",
    "    return stop_word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords_make(vocab_path_old = \"\", extend_stopwords = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "# from nltk.corpus import stopwords\n",
    "# stop_words = stopwords.words('english')\n",
    "# stop_words.extend(['one', 'two', 'three', 'amp', 'may', 'can', 'new', 'also', 'and'])\n",
    "\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "def word_process(tt):\n",
    "    \"\"\"\n",
    "    helper function to lower text, remove stop words, numbers, and empty \n",
    "    \"\"\"\n",
    "    \n",
    "    tt = tt.lower()\n",
    "    \n",
    "    punc = '''!()-[]{};:'\"\\, <>./?@#$%^&*_~=\\n'''\n",
    "    # Removing punctuations in string \n",
    "    # Using loop + punctuation string \n",
    "\n",
    "\n",
    "    for ele in tt:  \n",
    "        if ele in punc:  \n",
    "            tt = tt.replace(ele, \" \")  \n",
    "\n",
    "    # read tokens\n",
    "    tokens = tt.split()\n",
    "    lst = [token.translate(punc).lower() for token in tokens ]\n",
    "    \n",
    "    #remove stop words\n",
    "    filtered = []\n",
    "    for i in lst:\n",
    "        if i not in stop_words:\n",
    "            filtered.append(i)\n",
    "    \n",
    "    # removing singular numbers and singular letters\n",
    "    pattern = '[0-9]'\n",
    "    filtered = [re.sub(pattern, '', i) for i in filtered] \n",
    "    new = []\n",
    "    for inp in filtered:\n",
    "        new.append(' '.join( [w for w in inp.split() if len(w)>1] ))\n",
    "        \n",
    "    # filter out empty strings \n",
    "    new = [i for i in new if i] \n",
    "\n",
    "    dt = [d.split() for d in new]\n",
    "    \n",
    "    return dt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_cult['text_no_tags'] = tags_removed\n",
    "mm_demog['text_no_tags'] = tags_removed_demog\n",
    "mm_orgs['text_no_tags'] = tags_removed_org\n",
    "mm_rela['text_no_tags'] = tags_removed_rela\n",
    "\n",
    "mm_cult['processed'] = [word_process(i) for i in mm_cult['text_no_tags']]\n",
    "mm_demog['processed'] = [word_process(i) for i in mm_demog['text_no_tags']]\n",
    "mm_orgs['processed'] = [word_process(i) for i in mm_orgs['text_no_tags']]\n",
    "mm_rela['processed'] = [word_process(i) for i in mm_rela['text_no_tags']]\n",
    "mm_rela['processed'] = [sum(i, []) for i in mm_rela['processed']]\n",
    "mm_orgs['processed'] = [sum(i, []) for i in mm_orgs['processed']]\n",
    "mm_demog['processed'] = [sum(i, []) for i in mm_demog['processed']]\n",
    "mm_cult['processed'] = [sum(i, []) for i in mm_cult['processed']]\n",
    "combo_train_df = pd.concat([mm_rela, mm_orgs, mm_demog, mm_cult])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split datasets into decades & save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_decade = combo_train_df[combo_train_df['publicationYear'] <= 1979]\n",
    "second_decade = combo_train_df[(combo_train_df['publicationYear'] >= 1980) & (combo_train_df['publicationYear'] <= 1989) ]\n",
    "\n",
    "third_decade = combo_train_df[(combo_train_df['publicationYear'] >= 1990) & (combo_train_df['publicationYear'] <= 1999) ]\n",
    "\n",
    "\n",
    "fourth_decade = combo_train_df[(combo_train_df['publicationYear'] >= 2000) & (combo_train_df['publicationYear'] <= 2015) ]\n",
    "\n",
    "\n",
    "first_decade.to_csv('first_decade.csv')\n",
    "second_decade.to_csv('second_decade.csv')\n",
    "third_decade.to_csv('third_decade.csv')\n",
    "fourth_decade.to_csv('fourth_decade.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess text for each decade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove surnames, use enchant library to filter out non-English words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove surnames\n",
    "\n",
    "my_file = open(\"surnames.txt\", \"r\")\n",
    "data = my_file.read()\n",
    "surnames = data.split(\",\")\n",
    "surnames = [i.replace(\"'\", '').strip() for i in surnames]\n",
    "my_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pyenchant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import enchant\n",
    "valid_d = enchant.Dict(\"en_US\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_for_decade(decade_df, decade):\n",
    "    \"\"\"\n",
    "    remove surnames, filter by enchant dictionary, and filter by word length\n",
    "    \"\"\"\n",
    "    \n",
    "    p = [ast.literal_eval(i) for i in decade_df['processed']]\n",
    "    \n",
    "    processed_again = []\n",
    "\n",
    "    for i in tqdm(p):\n",
    "        k = [j for j in i if j not in surnames]\n",
    "        processed_again.append(k)\n",
    "        \n",
    "    processed_twice = []\n",
    "    for i in tqdm(processed_again):\n",
    "        k = [el for el in i if el.isalpha() and valid_d.check(el)]\n",
    "        processed_twice.append(k)\n",
    "    \n",
    "    for k in processed_twice:\n",
    "        processed_again2.append([i for i in k if len(i)>2])\n",
    "    \n",
    "    with open('processed_corp_enchant_' + decade +'.pkl', 'wb') as f:\n",
    "        pickle.dump(processed_twice, f)\n",
    "    \n",
    "    return processed_twice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_first_decade = process_for_decade(first_decade, 'first')\n",
    "processed_second_decade = process_for_decade(second_decade, 'second')\n",
    "processed_third_decade = process_for_decade(third_decade, 'third')\n",
    "processed_fourth_decade = process_for_decade(fourth_decade, 'fourth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train gensim phrased word2vec models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## build bigrams to create phased w2v models\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "def build_phrases(sentences):\n",
    "    phrases = Phrases(sentences, min_count = 5, threshold = 7, progress_per = 1000)\n",
    "    return Phraser(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram1 = build_phrases(processed_first_decade)\n",
    "bigram2 = build_phrases(processed_second_decade)\n",
    "bigram3 = build_phrases(processed_third_decade)\n",
    "bigram4 = build_phrases(processed_fourth_decade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_bigrams1 = [bigram1[i] for i in processed_first_decade]\n",
    "processed_bigrams2 = [bigram2[i] for i in processed_second_decade]\n",
    "processed_bigrams3 = [bigram3[i] for i in processed_third_decade]\n",
    "processed_bigrams4 = [bigram4[i] for i in processed_fourth_decade]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_bigrams_final1 = []\n",
    "processed_bigrams_final2 = []\n",
    "processed_bigrams_final3 = []\n",
    "processed_bigrams_final4 = []\n",
    "\n",
    "## strip punctuations\n",
    "for k in processed_bigrams1:\n",
    "    processed_bigrams_final1.append([i.strip('!\"“#$%&\\'()*+,-./:;<=>?@[\\\\]^`{|}~…') for i in k if len(i)>2])\n",
    "\n",
    "for k in processed_bigrams2:\n",
    "    processed_bigrams_final2.append([i.strip('!\"“#$%&\\'()*+,-./:;<=>?@[\\\\]^`{|}~…') for i in k if len(i)>2])\n",
    "\n",
    "for k in processed_bigrams3:\n",
    "    processed_bigrams_final3.append([i.strip('!\"“#$%&\\'()*+,-./:;<=>?@[\\\\]^`{|}~…') for i in k if len(i)>2])\n",
    "\n",
    "for k in processed_bigrams4:\n",
    "    processed_bigrams_final4.append([i.strip('!\"“#$%&\\'()*+,-./:;<=>?@[\\\\]^`{|}~…') for i in k if len(i)>2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from sklearn import utils\n",
    "cores = multiprocessing.cpu_count()\n",
    "import gensim\n",
    "from gensim.test.utils import get_tmpfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_decade_1 = gensim.models.Word2Vec(processed_bigrams_final1, vector_size = 300, window = 10,\n",
    "                                       min_count = 5, sg = 1, alpha = 0.05, epochs = 50, \n",
    "                                       batch_words = 10000, workers = cores, seed = 0, negative = 5,ns_exponent = 0.75)\n",
    "\n",
    "\n",
    "\n",
    "fname = \"word2vec_phrased_filtered_enchant_300d_1970_1979_2022_apr4.bin\"\n",
    "model_decade_1.save(fname)\n",
    "print(\"Model Saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_decade_2 = gensim.models.Word2Vec(processed_bigrams_final2, vector_size = 300, window = 10,\n",
    "                                       min_count = 5, sg = 1, alpha = 0.05, epochs = 50, \n",
    "                                       batch_words = 10000, workers = cores, seed = 0, negative = 5,ns_exponent = 0.75)\n",
    "\n",
    "\n",
    "\n",
    "fname = \"word2vec_phrased_filtered_enchant_300d_1980_1989_2022_apr4.bin\"\n",
    "model_decade_2.save(fname)\n",
    "print(\"Model Saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_decade_3 = gensim.models.Word2Vec(processed_bigrams_final3, vector_size = 300, window = 10,\n",
    "                                       min_count = 5, sg = 1, alpha = 0.05, epochs = 50, \n",
    "                                       batch_words = 10000, workers = cores, seed = 0, negative = 5,ns_exponent = 0.75)\n",
    "\n",
    "\n",
    "\n",
    "fname = \"word2vec_phrased_filtered_enchant_300d_1990_1999_2022_apr4.bin\"\n",
    "model_decade_3.save(fname)\n",
    "print(\"Model Saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_decade_4 = gensim.models.Word2Vec(processed_bigrams_final4, vector_size = 300, window = 10,\n",
    "                                       min_count = 5, sg = 1, alpha = 0.05, epochs = 50, \n",
    "                                       batch_words = 10000, workers = cores, seed = 0, negative = 5,ns_exponent = 0.75)\n",
    "\n",
    "\n",
    "\n",
    "fname = \"word2vec_phrased_filtered_enchant_300d_2000_2016_2022_apr4.bin\"\n",
    "model_decade_4.save(fname)\n",
    "print(\"Model Saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
