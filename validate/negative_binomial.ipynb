{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-8976fd11ee73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../../../models_storage/word_embeddings_data/ocr_text_with_tags_10000_jan21.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "df = pd.read_csv(\"../../../models_storage/word_embeddings_data/ocr_text_with_tags_10000_jan21.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>filename</th>\n",
       "      <th>text</th>\n",
       "      <th>edited_filename</th>\n",
       "      <th>text_unpacked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>journal-article-10.2307_2065002</td>\n",
       "      <td>['research', 'note', 'church', 'membership', '...</td>\n",
       "      <td>10.2307_2065002</td>\n",
       "      <td>research note church membership netherlands ro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>journal-article-10.2307_3380821</td>\n",
       "      <td>['polish', 'i3o', 'oo', 'sociological', 'revie...</td>\n",
       "      <td>10.2307_3380821</td>\n",
       "      <td>polish i3o oo sociological review issn communi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>journal-article-10.2307_2095822</td>\n",
       "      <td>['article', 'jjdlbsj', 'grapliy', 'compassiona...</td>\n",
       "      <td>10.2307_2095822</td>\n",
       "      <td>article jjdlbsj grapliy compassionate egalitar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>journal-article-10.2307_2631839</td>\n",
       "      <td>['reply', 'allison', 'more', 'comparing', 'reg...</td>\n",
       "      <td>10.2307_2631839</td>\n",
       "      <td>reply allison more comparing regression coeffi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>journal-article-10.2307_40836133</td>\n",
       "      <td>['determinants', 'spousal', 'interaction', 'ma...</td>\n",
       "      <td>10.2307_40836133</td>\n",
       "      <td>determinants spousal interaction marital struc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                          filename  \\\n",
       "0           0   journal-article-10.2307_2065002   \n",
       "1           1   journal-article-10.2307_3380821   \n",
       "2           2   journal-article-10.2307_2095822   \n",
       "3           3   journal-article-10.2307_2631839   \n",
       "4           4  journal-article-10.2307_40836133   \n",
       "\n",
       "                                                text   edited_filename  \\\n",
       "0  ['research', 'note', 'church', 'membership', '...   10.2307_2065002   \n",
       "1  ['polish', 'i3o', 'oo', 'sociological', 'revie...   10.2307_3380821   \n",
       "2  ['article', 'jjdlbsj', 'grapliy', 'compassiona...   10.2307_2095822   \n",
       "3  ['reply', 'allison', 'more', 'comparing', 'reg...   10.2307_2631839   \n",
       "4  ['determinants', 'spousal', 'interaction', 'ma...  10.2307_40836133   \n",
       "\n",
       "                                       text_unpacked  \n",
       "0  research note church membership netherlands ro...  \n",
       "1  polish i3o oo sociological review issn communi...  \n",
       "2  article jjdlbsj grapliy compassionate egalitar...  \n",
       "3  reply allison more comparing regression coeffi...  \n",
       "4  determinants spousal interaction marital struc...  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-103-7c387d23ed4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtext_unpacked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mliteral_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtext_unpacked_short\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext_unpacked\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-103-7c387d23ed4a>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtext_unpacked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mliteral_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtext_unpacked_short\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext_unpacked\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/ast.py\u001b[0m in \u001b[0;36mliteral_eval\u001b[0;34m(node_or_string)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \"\"\"\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_or_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mnode_or_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_or_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'eval'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_or_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExpression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mnode_or_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode_or_string\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/ast.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(source, filename, mode)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mEquivalent\u001b[0m \u001b[0mto\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPyCF_ONLY_AST\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \"\"\"\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPyCF_ONLY_AST\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "text_unpacked = [ast.literal_eval(t) for t in df.text]\n",
    "text_unpacked_short = [t[:2000] for t in text_unpacked]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.text_unpacked = [' '.join(t) for t in text_unpacked_short]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.discrete.discrete_model import NegativeBinomial\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv('../../../models_storage/word_embeddings_data/text_with_cosine_scores_wdg_jan11.csv')\n",
    "df = pd.read_csv('../../../models_storage/word_embeddings_data/text_with_cosine_scores_wdg_aug16.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_label = pd.read_csv('../../../models_storage/word_embeddings_data/counts_and_subject.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['article_id',\n",
       " 'culture_ngram_count',\n",
       " 'culture_ngram_count.1',\n",
       " 'relational_ngram_count',\n",
       " 'relational_ngram_count.1',\n",
       " 'demographic_ngram_count',\n",
       " 'demographic_ngram_count.1',\n",
       " 'word_count',\n",
       " 'cultural_author_count',\n",
       " 'demographic_author_count',\n",
       " 'relational_author_count',\n",
       " 'primary_subject',\n",
       " 'year']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_label.article_id = df_label.article_id.apply(lambda x: x[16:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, df_label, how='left', left_on='edited_filename', right_on='article_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.culture_ngram_count.isnull() == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['Unnamed: 0', 'Unnamed: 0_x', 'Unnamed: 0.1', 'Unnamed: 0_y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename_x</th>\n",
       "      <th>edited_filename</th>\n",
       "      <th>culture</th>\n",
       "      <th>demographic</th>\n",
       "      <th>relational</th>\n",
       "      <th>filename_y</th>\n",
       "      <th>text</th>\n",
       "      <th>relational_doc2vec_cosine</th>\n",
       "      <th>demographic_doc2vec_cosine</th>\n",
       "      <th>culture_doc2vec_cosine</th>\n",
       "      <th>...</th>\n",
       "      <th>relational_ngram_count</th>\n",
       "      <th>relational_ngram_count.1</th>\n",
       "      <th>demographic_ngram_count</th>\n",
       "      <th>demographic_ngram_count.1</th>\n",
       "      <th>word_count</th>\n",
       "      <th>cultural_author_count</th>\n",
       "      <th>demographic_author_count</th>\n",
       "      <th>relational_author_count</th>\n",
       "      <th>primary_subject</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../../../jstor_data/ocr/journal-article-10.230...</td>\n",
       "      <td>10.2307_1387034</td>\n",
       "      <td>0.609090</td>\n",
       "      <td>0.616106</td>\n",
       "      <td>0.450964</td>\n",
       "      <td>../../../jstor_data/ocr/journal-article-10.230...</td>\n",
       "      <td>research note church membership the netherland...</td>\n",
       "      <td>0.037736</td>\n",
       "      <td>0.074852</td>\n",
       "      <td>0.010707</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1361.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Sociology</td>\n",
       "      <td>1988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../../../jstor_data/ocr/journal-article-10.230...</td>\n",
       "      <td>10.2307_41274754</td>\n",
       "      <td>0.612232</td>\n",
       "      <td>0.618713</td>\n",
       "      <td>0.445802</td>\n",
       "      <td>../../../jstor_data/ocr/journal-article-10.230...</td>\n",
       "      <td>polish iooo sociological review   graduate sch...</td>\n",
       "      <td>0.028283</td>\n",
       "      <td>0.021214</td>\n",
       "      <td>0.038979</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1238.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Sociology</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../../../jstor_data/ocr/journal-article-10.230...</td>\n",
       "      <td>10.2307_24467156</td>\n",
       "      <td>0.618802</td>\n",
       "      <td>0.627400</td>\n",
       "      <td>0.459231</td>\n",
       "      <td>../../../jstor_data/ocr/journal-article-10.230...</td>\n",
       "      <td>article ■jjdlbsj grapliy compassionate egalita...</td>\n",
       "      <td>0.021041</td>\n",
       "      <td>0.025699</td>\n",
       "      <td>0.053906</td>\n",
       "      <td>...</td>\n",
       "      <td>46.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7959.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Sociology</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../../../jstor_data/ocr/journal-article-10.230...</td>\n",
       "      <td>10.2307_2782279</td>\n",
       "      <td>0.605174</td>\n",
       "      <td>0.611529</td>\n",
       "      <td>0.443576</td>\n",
       "      <td>../../../jstor_data/ocr/journal-article-10.230...</td>\n",
       "      <td>allison regressiallison  raises legitimate que...</td>\n",
       "      <td>0.019344</td>\n",
       "      <td>0.028638</td>\n",
       "      <td>0.029558</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1979.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Sociology</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>../../../jstor_data/ocr/journal-article-10.230...</td>\n",
       "      <td>10.2307_351656</td>\n",
       "      <td>0.613089</td>\n",
       "      <td>0.618886</td>\n",
       "      <td>0.450866</td>\n",
       "      <td>../../../jstor_data/ocr/journal-article-10.230...</td>\n",
       "      <td>determinants spousal interaction marital struc...</td>\n",
       "      <td>0.002458</td>\n",
       "      <td>0.050677</td>\n",
       "      <td>0.023544</td>\n",
       "      <td>...</td>\n",
       "      <td>14.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4340.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Sociology</td>\n",
       "      <td>1983</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          filename_x   edited_filename  \\\n",
       "0  ../../../jstor_data/ocr/journal-article-10.230...   10.2307_1387034   \n",
       "1  ../../../jstor_data/ocr/journal-article-10.230...  10.2307_41274754   \n",
       "2  ../../../jstor_data/ocr/journal-article-10.230...  10.2307_24467156   \n",
       "3  ../../../jstor_data/ocr/journal-article-10.230...   10.2307_2782279   \n",
       "6  ../../../jstor_data/ocr/journal-article-10.230...    10.2307_351656   \n",
       "\n",
       "    culture  demographic  relational  \\\n",
       "0  0.609090     0.616106    0.450964   \n",
       "1  0.612232     0.618713    0.445802   \n",
       "2  0.618802     0.627400    0.459231   \n",
       "3  0.605174     0.611529    0.443576   \n",
       "6  0.613089     0.618886    0.450866   \n",
       "\n",
       "                                          filename_y  \\\n",
       "0  ../../../jstor_data/ocr/journal-article-10.230...   \n",
       "1  ../../../jstor_data/ocr/journal-article-10.230...   \n",
       "2  ../../../jstor_data/ocr/journal-article-10.230...   \n",
       "3  ../../../jstor_data/ocr/journal-article-10.230...   \n",
       "6  ../../../jstor_data/ocr/journal-article-10.230...   \n",
       "\n",
       "                                                text  \\\n",
       "0  research note church membership the netherland...   \n",
       "1  polish iooo sociological review   graduate sch...   \n",
       "2  article ■jjdlbsj grapliy compassionate egalita...   \n",
       "3  allison regressiallison  raises legitimate que...   \n",
       "6  determinants spousal interaction marital struc...   \n",
       "\n",
       "   relational_doc2vec_cosine  demographic_doc2vec_cosine  \\\n",
       "0                   0.037736                    0.074852   \n",
       "1                   0.028283                    0.021214   \n",
       "2                   0.021041                    0.025699   \n",
       "3                   0.019344                    0.028638   \n",
       "6                   0.002458                    0.050677   \n",
       "\n",
       "   culture_doc2vec_cosine  ...  relational_ngram_count  \\\n",
       "0                0.010707  ...                     1.0   \n",
       "1                0.038979  ...                     5.0   \n",
       "2                0.053906  ...                    46.0   \n",
       "3                0.029558  ...                     0.0   \n",
       "6                0.023544  ...                    14.0   \n",
       "\n",
       "   relational_ngram_count.1  demographic_ngram_count  \\\n",
       "0                       0.0                      1.0   \n",
       "1                       2.0                      1.0   \n",
       "2                      21.0                      5.0   \n",
       "3                       0.0                      7.0   \n",
       "6                       6.0                     10.0   \n",
       "\n",
       "   demographic_ngram_count.1  word_count  cultural_author_count  \\\n",
       "0                        1.0      1361.0                    0.0   \n",
       "1                        1.0      1238.0                    0.0   \n",
       "2                        4.0      7959.0                    0.0   \n",
       "3                        7.0      1979.0                    0.0   \n",
       "6                        5.0      4340.0                    0.0   \n",
       "\n",
       "  demographic_author_count  relational_author_count  primary_subject  year  \n",
       "0                      0.0                      0.0        Sociology  1988  \n",
       "1                      0.0                      0.0        Sociology  2000  \n",
       "2                      0.0                      0.0        Sociology  2014  \n",
       "3                      0.0                      0.0        Sociology  1995  \n",
       "6                      0.0                      0.0        Sociology  1983  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert ngram counts to proportions, keep for logit models\n",
    "df['culture_ngram_prop'] = (df.culture_ngram_count)/(df.word_count)\n",
    "df['demographic_ngram_prop'] = (df.demographic_ngram_count)/(df.word_count)\n",
    "df['relational_ngram_prop'] = (df.relational_ngram_count)/(df.word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take log of word count (length matters less as it gets larger)\n",
    "df.word_count = np.log(df.word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discipline_to_numeric(x):\n",
    "    if x=='Sociology':\n",
    "        return 0\n",
    "    if x=='Management & Organizational Behavior':\n",
    "        return 1\n",
    "df.primary_subject = df.primary_subject.apply(discipline_to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.year = df.year.apply(lambda x: int(x[:4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.year = df.year.apply(lambda x: int(x[:4]) - int(1970))\n",
    "#df.year = (df.year - df.year.min())/(df.year.max() - df.year.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dichotomous coding of foundational text citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cultural_author_count'] = df['cultural_author_count'].apply(lambda x: 1 if x > 0 else 0)\n",
    "df['demographic_author_count'] = df['demographic_author_count'].apply(lambda x: 1 if x > 0 else 0)\n",
    "df['relational_author_count'] = df['relational_author_count'].apply(lambda x: 1 if x > 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename_x</th>\n",
       "      <th>edited_filename</th>\n",
       "      <th>culture</th>\n",
       "      <th>demographic</th>\n",
       "      <th>relational</th>\n",
       "      <th>filename_y</th>\n",
       "      <th>text</th>\n",
       "      <th>relational_doc2vec_cosine</th>\n",
       "      <th>demographic_doc2vec_cosine</th>\n",
       "      <th>culture_doc2vec_cosine</th>\n",
       "      <th>...</th>\n",
       "      <th>demographic_ngram_count.1</th>\n",
       "      <th>word_count</th>\n",
       "      <th>cultural_author_count</th>\n",
       "      <th>demographic_author_count</th>\n",
       "      <th>relational_author_count</th>\n",
       "      <th>primary_subject</th>\n",
       "      <th>year</th>\n",
       "      <th>culture_ngram_prop</th>\n",
       "      <th>demographic_ngram_prop</th>\n",
       "      <th>relational_ngram_prop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../../../jstor_data/ocr/journal-article-10.230...</td>\n",
       "      <td>10.2307_1387034</td>\n",
       "      <td>0.609090</td>\n",
       "      <td>0.616106</td>\n",
       "      <td>0.450964</td>\n",
       "      <td>../../../jstor_data/ocr/journal-article-10.230...</td>\n",
       "      <td>research note church membership the netherland...</td>\n",
       "      <td>0.037736</td>\n",
       "      <td>0.074852</td>\n",
       "      <td>0.010707</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.215975</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1988</td>\n",
       "      <td>0.000735</td>\n",
       "      <td>0.000735</td>\n",
       "      <td>0.000735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../../../jstor_data/ocr/journal-article-10.230...</td>\n",
       "      <td>10.2307_41274754</td>\n",
       "      <td>0.612232</td>\n",
       "      <td>0.618713</td>\n",
       "      <td>0.445802</td>\n",
       "      <td>../../../jstor_data/ocr/journal-article-10.230...</td>\n",
       "      <td>polish iooo sociological review   graduate sch...</td>\n",
       "      <td>0.028283</td>\n",
       "      <td>0.021214</td>\n",
       "      <td>0.038979</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.121252</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000808</td>\n",
       "      <td>0.004039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../../../jstor_data/ocr/journal-article-10.230...</td>\n",
       "      <td>10.2307_24467156</td>\n",
       "      <td>0.618802</td>\n",
       "      <td>0.627400</td>\n",
       "      <td>0.459231</td>\n",
       "      <td>../../../jstor_data/ocr/journal-article-10.230...</td>\n",
       "      <td>article ■jjdlbsj grapliy compassionate egalita...</td>\n",
       "      <td>0.021041</td>\n",
       "      <td>0.025699</td>\n",
       "      <td>0.053906</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.982059</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.001131</td>\n",
       "      <td>0.000628</td>\n",
       "      <td>0.005780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../../../jstor_data/ocr/journal-article-10.230...</td>\n",
       "      <td>10.2307_2782279</td>\n",
       "      <td>0.605174</td>\n",
       "      <td>0.611529</td>\n",
       "      <td>0.443576</td>\n",
       "      <td>../../../jstor_data/ocr/journal-article-10.230...</td>\n",
       "      <td>allison regressiallison  raises legitimate que...</td>\n",
       "      <td>0.019344</td>\n",
       "      <td>0.028638</td>\n",
       "      <td>0.029558</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.590347</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1995</td>\n",
       "      <td>0.012633</td>\n",
       "      <td>0.003537</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>../../../jstor_data/ocr/journal-article-10.230...</td>\n",
       "      <td>10.2307_351656</td>\n",
       "      <td>0.613089</td>\n",
       "      <td>0.618886</td>\n",
       "      <td>0.450866</td>\n",
       "      <td>../../../jstor_data/ocr/journal-article-10.230...</td>\n",
       "      <td>determinants spousal interaction marital struc...</td>\n",
       "      <td>0.002458</td>\n",
       "      <td>0.050677</td>\n",
       "      <td>0.023544</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.375630</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1983</td>\n",
       "      <td>0.005991</td>\n",
       "      <td>0.002304</td>\n",
       "      <td>0.003226</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          filename_x   edited_filename  \\\n",
       "0  ../../../jstor_data/ocr/journal-article-10.230...   10.2307_1387034   \n",
       "1  ../../../jstor_data/ocr/journal-article-10.230...  10.2307_41274754   \n",
       "2  ../../../jstor_data/ocr/journal-article-10.230...  10.2307_24467156   \n",
       "3  ../../../jstor_data/ocr/journal-article-10.230...   10.2307_2782279   \n",
       "6  ../../../jstor_data/ocr/journal-article-10.230...    10.2307_351656   \n",
       "\n",
       "    culture  demographic  relational  \\\n",
       "0  0.609090     0.616106    0.450964   \n",
       "1  0.612232     0.618713    0.445802   \n",
       "2  0.618802     0.627400    0.459231   \n",
       "3  0.605174     0.611529    0.443576   \n",
       "6  0.613089     0.618886    0.450866   \n",
       "\n",
       "                                          filename_y  \\\n",
       "0  ../../../jstor_data/ocr/journal-article-10.230...   \n",
       "1  ../../../jstor_data/ocr/journal-article-10.230...   \n",
       "2  ../../../jstor_data/ocr/journal-article-10.230...   \n",
       "3  ../../../jstor_data/ocr/journal-article-10.230...   \n",
       "6  ../../../jstor_data/ocr/journal-article-10.230...   \n",
       "\n",
       "                                                text  \\\n",
       "0  research note church membership the netherland...   \n",
       "1  polish iooo sociological review   graduate sch...   \n",
       "2  article ■jjdlbsj grapliy compassionate egalita...   \n",
       "3  allison regressiallison  raises legitimate que...   \n",
       "6  determinants spousal interaction marital struc...   \n",
       "\n",
       "   relational_doc2vec_cosine  demographic_doc2vec_cosine  \\\n",
       "0                   0.037736                    0.074852   \n",
       "1                   0.028283                    0.021214   \n",
       "2                   0.021041                    0.025699   \n",
       "3                   0.019344                    0.028638   \n",
       "6                   0.002458                    0.050677   \n",
       "\n",
       "   culture_doc2vec_cosine  ...  demographic_ngram_count.1  word_count  \\\n",
       "0                0.010707  ...                        1.0    7.215975   \n",
       "1                0.038979  ...                        1.0    7.121252   \n",
       "2                0.053906  ...                        4.0    8.982059   \n",
       "3                0.029558  ...                        7.0    7.590347   \n",
       "6                0.023544  ...                        5.0    8.375630   \n",
       "\n",
       "   cultural_author_count  demographic_author_count  relational_author_count  \\\n",
       "0                      0                         0                        0   \n",
       "1                      0                         0                        0   \n",
       "2                      0                         0                        0   \n",
       "3                      0                         0                        0   \n",
       "6                      0                         0                        0   \n",
       "\n",
       "   primary_subject  year  culture_ngram_prop  demographic_ngram_prop  \\\n",
       "0                0  1988            0.000735                0.000735   \n",
       "1                0  2000            0.000000                0.000808   \n",
       "2                0  2014            0.001131                0.000628   \n",
       "3                0  1995            0.012633                0.003537   \n",
       "6                0  1983            0.005991                0.002304   \n",
       "\n",
       "   relational_ngram_prop  \n",
       "0               0.000735  \n",
       "1               0.004039  \n",
       "2               0.005780  \n",
       "3               0.000000  \n",
       "6               0.003226  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: word_count, dtype: float64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.word_count[df.word_count.isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['culture'] = (df.culture - df.culture.min())/(df.culture.max()-df.culture.min())\n",
    "df['demographic'] = (df.demographic - df.demographic.min())/(df.demographic.max()-df.demographic.min())\n",
    "df['relational'] = (df.relational - df.relational.min())/(df.relational.max()-df.relational.min())\n",
    "\n",
    "df['culture_doc2vec_cosine'] = (df.culture_doc2vec_cosine - df.culture_doc2vec_cosine.min())/(df.culture_doc2vec_cosine.max()-df.culture_doc2vec_cosine.min())\n",
    "df['demographic_doc2vec_cosine'] = (df.demographic_doc2vec_cosine - df.demographic_doc2vec_cosine.min())/(df.demographic_doc2vec_cosine.max()-df.demographic_doc2vec_cosine.min())\n",
    "df['relational_doc2vec_cosine'] = (df.relational_doc2vec_cosine - df.relational_doc2vec_cosine.min())/(df.relational_doc2vec_cosine.max()-df.relational_doc2vec_cosine.min())\n",
    "\n",
    "df['culture_word2vec_cosine'] = (df.culture_word2vec_cosine - df.culture_word2vec_cosine.min())/(df.culture_word2vec_cosine.max()-df.culture_word2vec_cosine.min())\n",
    "df['demographic_word2vec_cosine'] = (df.demographic_word2vec_cosine - df.demographic_word2vec_cosine.min())/(df.demographic_word2vec_cosine.max()-df.demographic_word2vec_cosine.min())\n",
    "df['relational_word2vec_cosine'] = (df.relational_word2vec_cosine - df.relational_word2vec_cosine.min())/(df.relational_word2vec_cosine.max()-df.relational_word2vec_cosine.min())\n",
    "\n",
    "df['culture_glove_cosine'] = (df.culture_glove_cosine - df.culture_glove_cosine.min())/(df.culture_glove_cosine.max()-df.culture_glove_cosine.min())\n",
    "df['demographic_glove_cosine'] = (df.demographic_glove_cosine - df.demographic_glove_cosine.min())/(df.demographic_glove_cosine.max()-df.demographic_glove_cosine.min())\n",
    "df['relational_glove_cosine'] = (df.relational_glove_cosine - df.relational_glove_cosine.min())/(df.relational_glove_cosine.max()-df.relational_glove_cosine.min())\n",
    "\n",
    "df['culture_ngram_count'] = (df.culture_ngram_count - df.culture_ngram_count.min())/(df.culture_ngram_count.max()-df.culture_ngram_count.min())\n",
    "df['demographic_ngram_count'] = (df.demographic_ngram_count - df.demographic_ngram_count.min())/(df.demographic_ngram_count.max()-df.demographic_ngram_count.min())\n",
    "df['relational_ngram_count'] = (df.relational_ngram_count - df.relational_ngram_count.min())/(df.relational_ngram_count.max()-df.relational_ngram_count.min())\n",
    "\n",
    "df['culture_ngram_prop'] = (df.culture_ngram_prop - df.culture_ngram_prop.min())/(df.culture_ngram_prop.max()-df.culture_ngram_prop.min())\n",
    "df['demographic_ngram_prop'] = (df.demographic_ngram_prop - df.demographic_ngram_prop.min())/(df.demographic_ngram_prop.max()-df.demographic_ngram_prop.min())\n",
    "df['relational_ngram_prop'] = (df.relational_ngram_prop - df.relational_ngram_prop.min())/(df.relational_ngram_prop.max()-df.relational_ngram_prop.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show number of non-zeroes in DV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1039"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum((df['demographic_author_count']) > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "916"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum((df['relational_author_count']) > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1526"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum((df['cultural_author_count']) > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69657"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test/train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.random.rand(len(df)) < 0.8\n",
    "df_train = df[mask]\n",
    "df_test = df[~mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename_x</th>\n",
       "      <th>edited_filename</th>\n",
       "      <th>culture</th>\n",
       "      <th>demographic</th>\n",
       "      <th>relational</th>\n",
       "      <th>filename_y</th>\n",
       "      <th>text</th>\n",
       "      <th>relational_doc2vec_cosine</th>\n",
       "      <th>demographic_doc2vec_cosine</th>\n",
       "      <th>culture_doc2vec_cosine</th>\n",
       "      <th>...</th>\n",
       "      <th>demographic_ngram_count.1</th>\n",
       "      <th>word_count</th>\n",
       "      <th>cultural_author_count</th>\n",
       "      <th>demographic_author_count</th>\n",
       "      <th>relational_author_count</th>\n",
       "      <th>primary_subject</th>\n",
       "      <th>year</th>\n",
       "      <th>culture_ngram_prop</th>\n",
       "      <th>demographic_ngram_prop</th>\n",
       "      <th>relational_ngram_prop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../../../jstor_data/ocr/journal-article-10.230...</td>\n",
       "      <td>10.2307_1387034</td>\n",
       "      <td>0.904367</td>\n",
       "      <td>0.904887</td>\n",
       "      <td>0.875815</td>\n",
       "      <td>../../../jstor_data/ocr/journal-article-10.230...</td>\n",
       "      <td>research note church membership the netherland...</td>\n",
       "      <td>0.148715</td>\n",
       "      <td>0.313257</td>\n",
       "      <td>0.037126</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.215975</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1988</td>\n",
       "      <td>0.006796</td>\n",
       "      <td>0.009552</td>\n",
       "      <td>0.007073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../../../jstor_data/ocr/journal-article-10.230...</td>\n",
       "      <td>10.2307_41274754</td>\n",
       "      <td>0.911813</td>\n",
       "      <td>0.911070</td>\n",
       "      <td>0.864581</td>\n",
       "      <td>../../../jstor_data/ocr/journal-article-10.230...</td>\n",
       "      <td>polish iooo sociological review   graduate sch...</td>\n",
       "      <td>0.111461</td>\n",
       "      <td>0.088769</td>\n",
       "      <td>0.135160</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.121252</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010501</td>\n",
       "      <td>0.038879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../../../jstor_data/ocr/journal-article-10.230...</td>\n",
       "      <td>10.2307_24467156</td>\n",
       "      <td>0.927383</td>\n",
       "      <td>0.931674</td>\n",
       "      <td>0.893807</td>\n",
       "      <td>../../../jstor_data/ocr/journal-article-10.230...</td>\n",
       "      <td>article ■jjdlbsj grapliy compassionate egalita...</td>\n",
       "      <td>0.082921</td>\n",
       "      <td>0.107541</td>\n",
       "      <td>0.186922</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.982059</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.010460</td>\n",
       "      <td>0.008167</td>\n",
       "      <td>0.055637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../../../jstor_data/ocr/journal-article-10.230...</td>\n",
       "      <td>10.2307_2782279</td>\n",
       "      <td>0.895088</td>\n",
       "      <td>0.894031</td>\n",
       "      <td>0.859735</td>\n",
       "      <td>../../../jstor_data/ocr/journal-article-10.230...</td>\n",
       "      <td>allison regressiallison  raises legitimate que...</td>\n",
       "      <td>0.076234</td>\n",
       "      <td>0.119842</td>\n",
       "      <td>0.102492</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.590347</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1995</td>\n",
       "      <td>0.116852</td>\n",
       "      <td>0.045983</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>../../../jstor_data/ocr/journal-article-10.230...</td>\n",
       "      <td>10.2307_351656</td>\n",
       "      <td>0.913844</td>\n",
       "      <td>0.911480</td>\n",
       "      <td>0.875602</td>\n",
       "      <td>../../../jstor_data/ocr/journal-article-10.230...</td>\n",
       "      <td>determinants spousal interaction marital struc...</td>\n",
       "      <td>0.009682</td>\n",
       "      <td>0.212081</td>\n",
       "      <td>0.081638</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.375630</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1983</td>\n",
       "      <td>0.055415</td>\n",
       "      <td>0.029954</td>\n",
       "      <td>0.031053</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          filename_x   edited_filename  \\\n",
       "0  ../../../jstor_data/ocr/journal-article-10.230...   10.2307_1387034   \n",
       "1  ../../../jstor_data/ocr/journal-article-10.230...  10.2307_41274754   \n",
       "2  ../../../jstor_data/ocr/journal-article-10.230...  10.2307_24467156   \n",
       "3  ../../../jstor_data/ocr/journal-article-10.230...   10.2307_2782279   \n",
       "6  ../../../jstor_data/ocr/journal-article-10.230...    10.2307_351656   \n",
       "\n",
       "    culture  demographic  relational  \\\n",
       "0  0.904367     0.904887    0.875815   \n",
       "1  0.911813     0.911070    0.864581   \n",
       "2  0.927383     0.931674    0.893807   \n",
       "3  0.895088     0.894031    0.859735   \n",
       "6  0.913844     0.911480    0.875602   \n",
       "\n",
       "                                          filename_y  \\\n",
       "0  ../../../jstor_data/ocr/journal-article-10.230...   \n",
       "1  ../../../jstor_data/ocr/journal-article-10.230...   \n",
       "2  ../../../jstor_data/ocr/journal-article-10.230...   \n",
       "3  ../../../jstor_data/ocr/journal-article-10.230...   \n",
       "6  ../../../jstor_data/ocr/journal-article-10.230...   \n",
       "\n",
       "                                                text  \\\n",
       "0  research note church membership the netherland...   \n",
       "1  polish iooo sociological review   graduate sch...   \n",
       "2  article ■jjdlbsj grapliy compassionate egalita...   \n",
       "3  allison regressiallison  raises legitimate que...   \n",
       "6  determinants spousal interaction marital struc...   \n",
       "\n",
       "   relational_doc2vec_cosine  demographic_doc2vec_cosine  \\\n",
       "0                   0.148715                    0.313257   \n",
       "1                   0.111461                    0.088769   \n",
       "2                   0.082921                    0.107541   \n",
       "3                   0.076234                    0.119842   \n",
       "6                   0.009682                    0.212081   \n",
       "\n",
       "   culture_doc2vec_cosine  ...  demographic_ngram_count.1  word_count  \\\n",
       "0                0.037126  ...                        1.0    7.215975   \n",
       "1                0.135160  ...                        1.0    7.121252   \n",
       "2                0.186922  ...                        4.0    8.982059   \n",
       "3                0.102492  ...                        7.0    7.590347   \n",
       "6                0.081638  ...                        5.0    8.375630   \n",
       "\n",
       "   cultural_author_count  demographic_author_count  relational_author_count  \\\n",
       "0                      0                         0                        0   \n",
       "1                      0                         0                        0   \n",
       "2                      0                         0                        0   \n",
       "3                      0                         0                        0   \n",
       "6                      0                         0                        0   \n",
       "\n",
       "   primary_subject  year  culture_ngram_prop  demographic_ngram_prop  \\\n",
       "0                0  1988            0.006796                0.009552   \n",
       "1                0  2000            0.000000                0.010501   \n",
       "2                0  2014            0.010460                0.008167   \n",
       "3                0  1995            0.116852                0.045983   \n",
       "6                0  1983            0.055415                0.029954   \n",
       "\n",
       "   relational_ngram_prop  \n",
       "0               0.007073  \n",
       "1               0.038879  \n",
       "2               0.055637  \n",
       "3               0.000000  \n",
       "6               0.031053  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IGNORE (not used) nbreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndict_methods = {\\n    'culture_infersent': [['cultural_author_count'],['culture', 'primary_subject']],\\n    'demographic_infersent': [['demographic_author_count'],['demographic', 'primary_subject']],\\n    'relational_infersent': [['relational_author_count'],['relational', 'primary_subject']],\\n    'culture_doc2vec': [['cultural_author_count'],['culture_doc2vec_cosine', 'primary_subject']],\\n    'demographic_doc2vec': [['demographic_author_count'],['demographic_doc2vec_cosine', 'primary_subject']],\\n    'relational_doc2vec': [['relational_author_count'],['relational_doc2vec_cosine', 'primary_subject']],\\n    'culture_word2vec': [['cultural_author_count'],['culture_word2vec_cosine', 'primary_subject']],\\n    'demographic_word2vec': [['demographic_author_count'],['demographic_word2vec_cosine', 'primary_subject']],\\n    'relational_word2vec': [['relational_author_count'],['relational_word2vec_cosine', 'primary_subject']],\\n    'culture_glove': [['cultural_author_count'],['culture_glove_cosine', 'primary_subject']],\\n    'demographic_glove': [['demographic_author_count'],['demographic_glove_cosine', 'primary_subject']],\\n    'relational_glove': [['relational_author_count'],['relational_glove_cosine', 'primary_subject']],    \\n    'culture_ngram': [['cultural_author_count'],['culture_ngram_count', 'primary_subject']],\\n    'demographic_ngram': [['demographic_author_count'],['demographic_ngram_count', 'primary_subject']],\\n    'relational_ngram': [['relational_author_count'],['relational_ngram_count', 'primary_subject']],    \\n}\\n\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "dict_methods = {\n",
    "    'culture_infersent': [['cultural_author_count'],['culture', 'primary_subject']],\n",
    "    'demographic_infersent': [['demographic_author_count'],['demographic', 'primary_subject']],\n",
    "    'relational_infersent': [['relational_author_count'],['relational', 'primary_subject']],\n",
    "    'culture_doc2vec': [['cultural_author_count'],['culture_doc2vec_cosine', 'primary_subject']],\n",
    "    'demographic_doc2vec': [['demographic_author_count'],['demographic_doc2vec_cosine', 'primary_subject']],\n",
    "    'relational_doc2vec': [['relational_author_count'],['relational_doc2vec_cosine', 'primary_subject']],\n",
    "    'culture_word2vec': [['cultural_author_count'],['culture_word2vec_cosine', 'primary_subject']],\n",
    "    'demographic_word2vec': [['demographic_author_count'],['demographic_word2vec_cosine', 'primary_subject']],\n",
    "    'relational_word2vec': [['relational_author_count'],['relational_word2vec_cosine', 'primary_subject']],\n",
    "    'culture_glove': [['cultural_author_count'],['culture_glove_cosine', 'primary_subject']],\n",
    "    'demographic_glove': [['demographic_author_count'],['demographic_glove_cosine', 'primary_subject']],\n",
    "    'relational_glove': [['relational_author_count'],['relational_glove_cosine', 'primary_subject']],    \n",
    "    'culture_ngram': [['cultural_author_count'],['culture_ngram_count', 'primary_subject']],\n",
    "    'demographic_ngram': [['demographic_author_count'],['demographic_ngram_count', 'primary_subject']],\n",
    "    'relational_ngram': [['relational_author_count'],['relational_ngram_count', 'primary_subject']],    \n",
    "}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor key in dict_methods.keys():\\n    poisson_training_results = sm.GLM(df_train[dict_methods[key][0]], df_train[dict_methods[key][1]], \\n                                  family=sm.families.Poisson()).fit()\\n    BB_LAMBDA = poisson_training_results.mu\\n    AUX_OLS_DEP = ((df_train[dict_methods[key][0]] - BB_LAMBDA)**2 \\n                                                        - df_train[dict_methods[key][0]]) / BB_LAMBDA\\n    ols_expr = \"\"\"AUX_OLS_DEP ~ BB_LAMBDA - 1\"\"\"\\n    d = {\\'AUX_OLS_DEP\\': AUX_OLS_DEP, \\'BB_LAMBDA\\': BB_LAMBDA}\\n    df_new = pd.DataFrame(data=d)\\n    aux_olsr_results = smf.ols(ols_expr, df_new).fit()\\n    nb2_training_results = sm.GLM(df_train[dict_methods[key][0]], df_train[dict_methods[key][1]], \\n                              offset=df_train[\\'word_count\\'],\\n                              family=sm.families.NegativeBinomial(alpha=aux_olsr_results.params[0])).fit()\\n    print(key)\\n    print(nb2_training_results.summary())\\n    print()\\n    '"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "for key in dict_methods.keys():\n",
    "    poisson_training_results = sm.GLM(df_train[dict_methods[key][0]], df_train[dict_methods[key][1]], \n",
    "                                  family=sm.families.Poisson()).fit()\n",
    "    BB_LAMBDA = poisson_training_results.mu\n",
    "    AUX_OLS_DEP = ((df_train[dict_methods[key][0]] - BB_LAMBDA)**2 \n",
    "                                                        - df_train[dict_methods[key][0]]) / BB_LAMBDA\n",
    "    ols_expr = \"\"\"AUX_OLS_DEP ~ BB_LAMBDA - 1\"\"\"\n",
    "    d = {'AUX_OLS_DEP': AUX_OLS_DEP, 'BB_LAMBDA': BB_LAMBDA}\n",
    "    df_new = pd.DataFrame(data=d)\n",
    "    aux_olsr_results = smf.ols(ols_expr, df_new).fit()\n",
    "    nb2_training_results = sm.GLM(df_train[dict_methods[key][0]], df_train[dict_methods[key][1]], \n",
    "                              offset=df_train['word_count'],\n",
    "                              family=sm.families.NegativeBinomial(alpha=aux_olsr_results.params[0])).fit()\n",
    "    print(key)\n",
    "    print(nb2_training_results.summary())\n",
    "    print()\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Reg\n",
    "## Infersent - Culture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(df_train[['culture']], df_train['cultural_author_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic regression classifier on test set: 0.98\n"
     ]
    }
   ],
   "source": [
    "y_pred = logreg.predict(df_test[['culture']])\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(df_test[['culture']]\n",
    "                                                                                           , df_test['cultural_author_count'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13562     0]\n",
      " [  310     0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(df_test['cultural_author_count'], y_pred)\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.101535\n",
      "         Iterations 8\n",
      "                            Results: Logit\n",
      "=======================================================================\n",
      "Model:              Logit                 Pseudo R-squared: 0.036      \n",
      "Dependent Variable: cultural_author_count AIC:              14151.2581 \n",
      "Date:               2020-01-29 12:54      BIC:              14178.7121 \n",
      "No. Observations:   69657                 Log-Likelihood:   -7072.6    \n",
      "Df Model:           2                     LL-Null:          -7339.9    \n",
      "Df Residuals:       69654                 LLR p-value:      8.3818e-117\n",
      "Converged:          1.0000                Scale:            1.0000     \n",
      "No. Iterations:     8.0000                                             \n",
      "------------------------------------------------------------------------\n",
      "                    Coef.   Std.Err.     z      P>|z|    [0.025   0.975]\n",
      "------------------------------------------------------------------------\n",
      "culture            -5.7096    0.5577  -10.2371  0.0000  -6.8027  -4.6165\n",
      "primary_subject     1.1291    0.0521   21.6892  0.0000   1.0271   1.2311\n",
      "year                0.0005    0.0003    1.8681  0.0617  -0.0000   0.0010\n",
      "=======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logit_model=sm.Logit(df['cultural_author_count'], df[['culture', 'primary_subject', 'year']])\n",
    "result=logit_model.fit()\n",
    "print(result.summary2())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(result.predict(df_train[['culture_glove_cosine', 'primary_subject', 'year']]) > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1216"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum((df_train['cultural_author_count']) > 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infersent - Demographic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.072251\n",
      "         Iterations 9\n",
      "                              Results: Logit\n",
      "==========================================================================\n",
      "Model:              Logit                    Pseudo R-squared: 0.068      \n",
      "Dependent Variable: demographic_author_count AIC:              10071.5756 \n",
      "Date:               2020-01-29 12:54         BIC:              10099.0296 \n",
      "No. Observations:   69657                    Log-Likelihood:   -5032.8    \n",
      "Df Model:           2                        LL-Null:          -5400.5    \n",
      "Df Residuals:       69654                    LLR p-value:      1.9288e-160\n",
      "Converged:          1.0000                   Scale:            1.0000     \n",
      "No. Iterations:     9.0000                                                \n",
      "----------------------------------------------------------------------------\n",
      "                    Coef.    Std.Err.      z      P>|z|     [0.025    0.975]\n",
      "----------------------------------------------------------------------------\n",
      "demographic        -4.3509     0.7556   -5.7583   0.0000   -5.8319   -2.8700\n",
      "primary_subject     1.6689     0.0663   25.1595   0.0000    1.5389    1.7989\n",
      "year               -0.0005     0.0003   -1.4707   0.1414   -0.0012    0.0002\n",
      "==========================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logit_model=sm.Logit(df['demographic_author_count'],df[['demographic', 'primary_subject', 'year']])\n",
    "result=logit_model.fit()\n",
    "print(result.summary2())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infersent - relational"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.062012\n",
      "         Iterations 10\n",
      "                             Results: Logit\n",
      "=========================================================================\n",
      "Model:              Logit                   Pseudo R-squared: 0.114      \n",
      "Dependent Variable: relational_author_count AIC:              8645.0881  \n",
      "Date:               2020-01-28 23:05        BIC:              8672.5421  \n",
      "No. Observations:   69657                   Log-Likelihood:   -4319.5    \n",
      "Df Model:           2                       LL-Null:          -4877.4    \n",
      "Df Residuals:       69654                   LLR p-value:      5.1066e-243\n",
      "Converged:          1.0000                  Scale:            1.0000     \n",
      "No. Iterations:     10.0000                                              \n",
      "--------------------------------------------------------------------------\n",
      "                   Coef.    Std.Err.     z      P>|z|    [0.025    0.975] \n",
      "--------------------------------------------------------------------------\n",
      "relational        -12.1845    0.2032  -59.9754  0.0000  -12.5827  -11.7863\n",
      "primary_subject     2.1973    0.0787   27.9079  0.0000    2.0430    2.3516\n",
      "year               -0.0817    0.1149   -0.7111  0.4770   -0.3069    0.1435\n",
      "=========================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logit_model=sm.Logit(df['relational_author_count'],df[['relational', 'primary_subject', 'year']])\n",
    "result=logit_model.fit()\n",
    "print(result.summary2())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2vec - culture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.157398\n",
      "         Iterations 9\n",
      "                              Results: Logit\n",
      "==========================================================================\n",
      "Model:                Logit                  Pseudo R-squared:  -0.494    \n",
      "Dependent Variable:   cultural_author_count  AIC:               21933.7478\n",
      "Date:                 2020-01-28 23:05       BIC:               21961.2018\n",
      "No. Observations:     69657                  Log-Likelihood:    -10964.   \n",
      "Df Model:             2                      LL-Null:           -7339.9   \n",
      "Df Residuals:         69654                  LLR p-value:       1.0000    \n",
      "Converged:            1.0000                 Scale:             1.0000    \n",
      "No. Iterations:       9.0000                                              \n",
      "--------------------------------------------------------------------------\n",
      "                        Coef.   Std.Err.    z     P>|z|   [0.025   0.975] \n",
      "--------------------------------------------------------------------------\n",
      "culture_doc2vec_cosine -46.0343   0.8062 -57.0989 0.0000 -47.6144 -44.4541\n",
      "primary_subject         -0.1862   0.0421  -4.4253 0.0000  -0.2686  -0.1037\n",
      "year                    -3.7444   0.0596 -62.7796 0.0000  -3.8613  -3.6275\n",
      "==========================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logit_model=sm.Logit(df['cultural_author_count'],df[['culture_doc2vec_cosine', 'primary_subject', 'year']])\n",
    "result=logit_model.fit()\n",
    "print(result.summary2())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2vec - demographic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.118219\n",
      "         Iterations 9\n",
      "                                Results: Logit\n",
      "==============================================================================\n",
      "Model:                 Logit                     Pseudo R-squared:  -0.525    \n",
      "Dependent Variable:    demographic_author_count  AIC:               16475.4992\n",
      "Date:                  2020-01-28 23:05          BIC:               16502.9532\n",
      "No. Observations:      69657                     Log-Likelihood:    -8234.7   \n",
      "Df Model:              2                         LL-Null:           -5400.5   \n",
      "Df Residuals:          69654                     LLR p-value:       1.0000    \n",
      "Converged:             1.0000                    Scale:             1.0000    \n",
      "No. Iterations:        9.0000                                                 \n",
      "------------------------------------------------------------------------------\n",
      "                            Coef.   Std.Err.    z     P>|z|   [0.025   0.975] \n",
      "------------------------------------------------------------------------------\n",
      "demographic_doc2vec_cosine -50.7661   0.9852 -51.5307 0.0000 -52.6970 -48.8352\n",
      "primary_subject              0.0909   0.0472   1.9267 0.0540  -0.0016   0.1833\n",
      "year                        -4.9944   0.0836 -59.7219 0.0000  -5.1583  -4.8305\n",
      "==============================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logit_model=sm.Logit(df['demographic_author_count'],df[['demographic_doc2vec_cosine', 'primary_subject', 'year']])\n",
    "result=logit_model.fit()\n",
    "print(result.summary2())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2vec - relational"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.107254\n",
      "         Iterations 9\n",
      "                               Results: Logit\n",
      "=============================================================================\n",
      "Model:                 Logit                    Pseudo R-squared:  -0.532    \n",
      "Dependent Variable:    relational_author_count  AIC:               14947.9429\n",
      "Date:                  2020-01-28 23:05         BIC:               14975.3970\n",
      "No. Observations:      69657                    Log-Likelihood:    -7471.0   \n",
      "Df Model:              2                        LL-Null:           -4877.4   \n",
      "Df Residuals:          69654                    LLR p-value:       1.0000    \n",
      "Converged:             1.0000                   Scale:             1.0000    \n",
      "No. Iterations:        9.0000                                                \n",
      "-----------------------------------------------------------------------------\n",
      "                           Coef.   Std.Err.    z     P>|z|   [0.025   0.975] \n",
      "-----------------------------------------------------------------------------\n",
      "relational_doc2vec_cosine -53.9535   1.0721 -50.3263 0.0000 -56.0547 -51.8523\n",
      "primary_subject             0.2709   0.0479   5.6496 0.0000   0.1769   0.3648\n",
      "year                       -5.4320   0.0935 -58.1030 0.0000  -5.6152  -5.2487\n",
      "=============================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logit_model=sm.Logit(df['relational_author_count'],df[['relational_doc2vec_cosine', 'primary_subject', 'year']])\n",
    "result=logit_model.fit()\n",
    "print(result.summary2())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec - culture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.118564\n",
      "         Iterations 8\n",
      "                             Results: Logit\n",
      "========================================================================\n",
      "Model:                Logit                 Pseudo R-squared: -0.125    \n",
      "Dependent Variable:   cultural_author_count AIC:              16523.6430\n",
      "Date:                 2020-01-28 23:05      BIC:              16551.0970\n",
      "No. Observations:     69657                 Log-Likelihood:   -8258.8   \n",
      "Df Model:             2                     LL-Null:          -7339.9   \n",
      "Df Residuals:         69654                 LLR p-value:      1.0000    \n",
      "Converged:            1.0000                Scale:            1.0000    \n",
      "No. Iterations:       8.0000                                            \n",
      "------------------------------------------------------------------------\n",
      "                         Coef.  Std.Err.    z     P>|z|   [0.025  0.975]\n",
      "------------------------------------------------------------------------\n",
      "culture_word2vec_cosine -6.5502   0.0887 -73.8207 0.0000 -6.7241 -6.3763\n",
      "primary_subject          0.7164   0.0471  15.2049 0.0000  0.6240  0.8087\n",
      "year                     0.0837   0.0800   1.0465 0.2953 -0.0731  0.2405\n",
      "========================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logit_model=sm.Logit(df['cultural_author_count'],df[['culture_word2vec_cosine', 'primary_subject', 'year']])\n",
    "result=logit_model.fit()\n",
    "print(result.summary2())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec - demographic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.084184\n",
      "         Iterations 9\n",
      "                               Results: Logit\n",
      "============================================================================\n",
      "Model:               Logit                     Pseudo R-squared:  -0.086    \n",
      "Dependent Variable:  demographic_author_count  AIC:               11733.9408\n",
      "Date:                2020-01-28 23:06          BIC:               11761.3948\n",
      "No. Observations:    69657                     Log-Likelihood:    -5864.0   \n",
      "Df Model:            2                         LL-Null:           -5400.5   \n",
      "Df Residuals:        69654                     LLR p-value:       1.0000    \n",
      "Converged:           1.0000                    Scale:             1.0000    \n",
      "No. Iterations:      9.0000                                                 \n",
      "----------------------------------------------------------------------------\n",
      "                             Coef.  Std.Err.    z     P>|z|   [0.025  0.975]\n",
      "----------------------------------------------------------------------------\n",
      "demographic_word2vec_cosine -6.8046   0.1017 -66.9030 0.0000 -7.0039 -6.6052\n",
      "primary_subject              1.4979   0.0591  25.3599 0.0000  1.3821  1.6136\n",
      "year                        -0.6982   0.0980  -7.1275 0.0000 -0.8902 -0.5062\n",
      "============================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logit_model=sm.Logit(df['demographic_author_count'],df[['demographic_word2vec_cosine', 'primary_subject', 'year']])\n",
    "result=logit_model.fit()\n",
    "print(result.summary2())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec - relational"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.073016\n",
      "         Iterations 9\n",
      "                              Results: Logit\n",
      "===========================================================================\n",
      "Model:               Logit                    Pseudo R-squared:  -0.043    \n",
      "Dependent Variable:  relational_author_count  AIC:               10178.2036\n",
      "Date:                2020-01-28 23:06         BIC:               10205.6577\n",
      "No. Observations:    69657                    Log-Likelihood:    -5086.1   \n",
      "Df Model:            2                        LL-Null:           -4877.4   \n",
      "Df Residuals:        69654                    LLR p-value:       1.0000    \n",
      "Converged:           1.0000                   Scale:             1.0000    \n",
      "No. Iterations:      9.0000                                                \n",
      "---------------------------------------------------------------------------\n",
      "                            Coef.  Std.Err.    z     P>|z|   [0.025  0.975]\n",
      "---------------------------------------------------------------------------\n",
      "relational_word2vec_cosine -6.8780   0.1073 -64.1148 0.0000 -7.0883 -6.6678\n",
      "primary_subject             1.9741   0.0658  30.0197 0.0000  1.8452  2.1030\n",
      "year                       -0.8359   0.1058  -7.9032 0.0000 -1.0432 -0.6286\n",
      "===========================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logit_model=sm.Logit(df['relational_author_count'],df[['relational_word2vec_cosine', 'primary_subject', 'year']])\n",
    "result=logit_model.fit()\n",
    "print(result.summary2())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove - culture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.114411\n",
      "         Iterations 8\n",
      "                            Results: Logit\n",
      "======================================================================\n",
      "Model:              Logit                 Pseudo R-squared: -0.086    \n",
      "Dependent Variable: cultural_author_count AIC:              15945.0233\n",
      "Date:               2020-01-28 23:06      BIC:              15972.4773\n",
      "No. Observations:   69657                 Log-Likelihood:   -7969.5   \n",
      "Df Model:           2                     LL-Null:          -7339.9   \n",
      "Df Residuals:       69654                 LLR p-value:      1.0000    \n",
      "Converged:          1.0000                Scale:            1.0000    \n",
      "No. Iterations:     8.0000                                            \n",
      "----------------------------------------------------------------------\n",
      "                       Coef.  Std.Err.    z     P>|z|   [0.025  0.975]\n",
      "----------------------------------------------------------------------\n",
      "culture_glove_cosine  -7.2710   0.0969 -75.0742 0.0000 -7.4608 -7.0811\n",
      "primary_subject        0.6808   0.0474  14.3599 0.0000  0.5879  0.7737\n",
      "year                   0.1635   0.0788   2.0750 0.0380  0.0091  0.3179\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logit_model=sm.Logit(df['cultural_author_count'],df[['culture_glove_cosine', 'primary_subject', 'year']])\n",
    "result=logit_model.fit()\n",
    "print(result.summary2())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove - demographic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.080611\n",
      "         Iterations 9\n",
      "                             Results: Logit\n",
      "=========================================================================\n",
      "Model:              Logit                    Pseudo R-squared: -0.040    \n",
      "Dependent Variable: demographic_author_count AIC:              11236.1835\n",
      "Date:               2020-01-28 23:06         BIC:              11263.6375\n",
      "No. Observations:   69657                    Log-Likelihood:   -5615.1   \n",
      "Df Model:           2                        LL-Null:          -5400.5   \n",
      "Df Residuals:       69654                    LLR p-value:      1.0000    \n",
      "Converged:          1.0000                   Scale:            1.0000    \n",
      "No. Iterations:     9.0000                                               \n",
      "-------------------------------------------------------------------------\n",
      "                          Coef.  Std.Err.    z     P>|z|   [0.025  0.975]\n",
      "-------------------------------------------------------------------------\n",
      "demographic_glove_cosine -6.5861   0.0976 -67.4487 0.0000 -6.7775 -6.3947\n",
      "primary_subject           1.3891   0.0590  23.5423 0.0000  1.2734  1.5047\n",
      "year                     -0.5653   0.0971  -5.8217 0.0000 -0.7557 -0.3750\n",
      "=========================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logit_model=sm.Logit(df['demographic_author_count'],df[['demographic_glove_cosine', 'primary_subject', 'year']])\n",
    "result=logit_model.fit()\n",
    "print(result.summary2())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove - relational"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.069880\n",
      "         Iterations 9\n",
      "                             Results: Logit\n",
      "========================================================================\n",
      "Model:              Logit                   Pseudo R-squared: 0.002     \n",
      "Dependent Variable: relational_author_count AIC:              9741.2114 \n",
      "Date:               2020-01-28 23:06        BIC:              9768.6654 \n",
      "No. Observations:   69657                   Log-Likelihood:   -4867.6   \n",
      "Df Model:           2                       LL-Null:          -4877.4   \n",
      "Df Residuals:       69654                   LLR p-value:      5.3490e-05\n",
      "Converged:          1.0000                  Scale:            1.0000    \n",
      "No. Iterations:     9.0000                                              \n",
      "------------------------------------------------------------------------\n",
      "                         Coef.  Std.Err.    z     P>|z|   [0.025  0.975]\n",
      "------------------------------------------------------------------------\n",
      "relational_glove_cosine -6.4927   0.1012 -64.1318 0.0000 -6.6911 -6.2942\n",
      "primary_subject          1.9149   0.0672  28.4978 0.0000  1.7832  2.0466\n",
      "year                    -0.7288   0.1052  -6.9269 0.0000 -0.9350 -0.5226\n",
      "========================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logit_model=sm.Logit(df['relational_author_count'],df[['relational_glove_cosine', 'primary_subject', 'year']])\n",
    "result=logit_model.fit()\n",
    "print(result.summary2())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ngram - culture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.202555\n",
      "         Iterations 9\n",
      "                            Results: Logit\n",
      "======================================================================\n",
      "Model:              Logit                 Pseudo R-squared: -0.922    \n",
      "Dependent Variable: cultural_author_count AIC:              28224.6936\n",
      "Date:               2020-01-28 23:06      BIC:              28252.1477\n",
      "No. Observations:   69657                 Log-Likelihood:   -14109.   \n",
      "Df Model:           2                     LL-Null:          -7339.9   \n",
      "Df Residuals:       69654                 LLR p-value:      1.0000    \n",
      "Converged:          1.0000                Scale:            1.0000    \n",
      "No. Iterations:     9.0000                                            \n",
      "----------------------------------------------------------------------\n",
      "                      Coef.  Std.Err.     z     P>|z|   [0.025  0.975]\n",
      "----------------------------------------------------------------------\n",
      "culture_ngram_count   0.0071   0.0004   17.2018 0.0000  0.0063  0.0079\n",
      "primary_subject      -1.0064   0.0411  -24.5150 0.0000 -1.0869 -0.9260\n",
      "year                 -7.3517   0.0720 -102.1668 0.0000 -7.4927 -7.2106\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logit_model=sm.Logit(df['cultural_author_count'],df[['culture_ngram_count', 'primary_subject', 'year']])\n",
    "result=logit_model.fit()\n",
    "print(result.summary2())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ngram - demographic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.155104\n",
      "         Iterations 9\n",
      "                             Results: Logit\n",
      "=========================================================================\n",
      "Model:              Logit                    Pseudo R-squared: -1.001    \n",
      "Dependent Variable: demographic_author_count AIC:              21614.1569\n",
      "Date:               2020-01-28 23:06         BIC:              21641.6109\n",
      "No. Observations:   69657                    Log-Likelihood:   -10804.   \n",
      "Df Model:           2                        LL-Null:          -5400.5   \n",
      "Df Residuals:       69654                    LLR p-value:      1.0000    \n",
      "Converged:          1.0000                   Scale:            1.0000    \n",
      "No. Iterations:     9.0000                                               \n",
      "-------------------------------------------------------------------------\n",
      "                         Coef.  Std.Err.    z     P>|z|   [0.025   0.975]\n",
      "-------------------------------------------------------------------------\n",
      "demographic_ngram_count  0.0116   0.0005  23.6999 0.0000   0.0106  0.0125\n",
      "primary_subject         -0.8938   0.0453 -19.7162 0.0000  -0.9827 -0.8050\n",
      "year                    -9.9731   0.1089 -91.6142 0.0000 -10.1865 -9.7598\n",
      "=========================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logit_model=sm.Logit(df['demographic_author_count'],df[['demographic_ngram_count', 'primary_subject', 'year']])\n",
    "result=logit_model.fit()\n",
    "print(result.summary2())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ngram - relational"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.147090\n",
      "         Iterations 9\n",
      "                             Results: Logit\n",
      "=========================================================================\n",
      "Model:               Logit                   Pseudo R-squared: -1.101    \n",
      "Dependent Variable:  relational_author_count AIC:              20497.7053\n",
      "Date:                2020-01-28 23:06        BIC:              20525.1594\n",
      "No. Observations:    69657                   Log-Likelihood:   -10246.   \n",
      "Df Model:            2                       LL-Null:          -4877.4   \n",
      "Df Residuals:        69654                   LLR p-value:      1.0000    \n",
      "Converged:           1.0000                  Scale:            1.0000    \n",
      "No. Iterations:      9.0000                                              \n",
      "-------------------------------------------------------------------------\n",
      "                        Coef.   Std.Err.    z     P>|z|   [0.025   0.975]\n",
      "-------------------------------------------------------------------------\n",
      "relational_ngram_count   0.0016   0.0003   5.4756 0.0000   0.0010  0.0021\n",
      "primary_subject         -0.6821   0.0465 -14.6765 0.0000  -0.7732 -0.5910\n",
      "year                   -10.1971   0.1196 -85.2943 0.0000 -10.4314 -9.9628\n",
      "=========================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logit_model=sm.Logit(df['relational_author_count'],df[['relational_ngram_count', 'primary_subject', 'year']])\n",
    "result=logit_model.fit()\n",
    "print(result.summary2())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#change this if you have more dictionary sizes\n",
    "ls_lens = [5, 10, 100, 5, 10, 100, 5, 10, 100, 5, 10, 100, 5, 10, 100]\n",
    "ls_avgs = [0.038, 0.068, 0.116, \n",
    "           0.031, 0.066, 0.104,\n",
    "           0.288, 0.438, 0.342,\n",
    "           0.132, 0.199, 0.294,\n",
    "           0.197, 0.278, 0.23\n",
    "          ] #Put three same things because we only have one dictionary size rn\n",
    "                                               #change later when dictionary sizes are provided and created\n",
    "d = {'Strength': ls_avgs, 'Words':ls_lens, 'Model':['InferSent', 'InferSent', 'InferSent',\n",
    "                                                    'Doc2Vec','Doc2Vec','Doc2Vec', \n",
    "                                                    'Word2Vec', 'Word2Vec', 'Word2Vec',\n",
    "                                                    'GloVe', 'GloVe', 'GloVe',\n",
    "                                                    'word count', 'word count', 'word count'\n",
    "                                                     ]}\n",
    "dat = pd.DataFrame(data=d)\n",
    "x1 = list(range(3)) #change this if you have more dictionary sizes\n",
    "squad = ['Cultural','Demographic','Relational'] #arbitrary values for demonstration purpose only atm.\n",
    "\n",
    "import seaborn as sns; sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_style(\"white\")\n",
    "ax = sns.pointplot(x=\"Words\", y=\"Strength\", data=dat, hue='Model')\n",
    "ax.set_xticks(x1)\n",
    "ax.set_xticklabels(squad, minor=False, rotation=30)\n",
    "ax.set_ylim(-0.05,1)\n",
    "ax.set(xlabel='', ylabel='Pseudo R-squared')\n",
    "plt.title(\"Proportion of Variance Explained: Logistic Regression Models\\n\")\n",
    "#plt.tight_layout()\n",
    "plt.gcf().subplots_adjust(bottom=0.19)\n",
    "plt.savefig('logistic_reg_binary_012620.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nbreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv('../../../models_storage/word_embeddings_data/text_with_cosine_scores_wdg_jan11.csv')\n",
    "df = pd.read_csv('../../../models_storage/word_embeddings_data/text_with_cosine_scores_wdg_aug16.csv')\n",
    "df_label = pd.read_csv('../../../models_storage/word_embeddings_data/counts_and_subject.csv', low_memory=False)\n",
    "df_label.article_id = df_label.article_id.apply(lambda x: x[16:])\n",
    "df = pd.merge(df, df_label, how='left', left_on='edited_filename', right_on='article_id')\n",
    "df = df[df.culture_ngram_count.isnull() == False]\n",
    "df = df.drop(columns=['Unnamed: 0', 'Unnamed: 0_x', 'Unnamed: 0.1', 'Unnamed: 0_y'])\n",
    "df.word_count = np.log(df.word_count)\n",
    "def discipline_to_numeric(x):\n",
    "    if x=='Sociology':\n",
    "        return 0\n",
    "    if x=='Management & Organizational Behavior':\n",
    "        return 1\n",
    "df.primary_subject = df.primary_subject.apply(discipline_to_numeric)\n",
    "df.year = df.year.apply(lambda x: int(x[:4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## infersent \n",
    "### Culture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "poisson_training_results = sm.GLM(df['cultural_author_count'], df[['culture', 'primary_subject']], \n",
    "                                  family=sm.families.Poisson()).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['BB_LAMBDA'] = poisson_training_results.mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['AUX_OLS_DEP'] = df.apply(lambda x: ((x['cultural_author_count'] - \n",
    "                                                     x['BB_LAMBDA'])**2 - x['cultural_author_count']) / x['BB_LAMBDA'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_expr = \"\"\"AUX_OLS_DEP ~ BB_LAMBDA - 1\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_olsr_results = smf.ols(ols_expr, df).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BB_LAMBDA    58.737906\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(aux_olsr_results.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BB_LAMBDA    7.487131\n",
       "dtype: float64"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aux_olsr_results.tvalues #98% confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb2_training_results = sm.GLM(df['cultural_author_count'], df[['culture', 'primary_subject']], \n",
    "                              offset=df['word_count'],\n",
    "                              family=sm.families.NegativeBinomial(alpha=aux_olsr_results.params[0])).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Generalized Linear Model Regression Results                   \n",
      "=================================================================================\n",
      "Dep. Variable:     cultural_author_count   No. Observations:                69657\n",
      "Model:                               GLM   Df Residuals:                    69655\n",
      "Model Family:           NegativeBinomial   Df Model:                            1\n",
      "Link Function:                       log   Scale:                          1.0000\n",
      "Method:                             IRLS   Log-Likelihood:                -9499.9\n",
      "Date:                   Tue, 28 Jan 2020   Deviance:                       4179.5\n",
      "Time:                           19:42:08   Pearson chi2:                 7.70e+04\n",
      "No. Iterations:                       10   Covariance Type:             nonrobust\n",
      "===================================================================================\n",
      "                      coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------\n",
      "culture           -19.8574      0.072   -273.943      0.000     -19.999     -19.715\n",
      "primary_subject     1.0283      0.077     13.298      0.000       0.877       1.180\n",
      "===================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(nb2_training_results.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demographic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Generalized Linear Model Regression Results                     \n",
      "====================================================================================\n",
      "Dep. Variable:     demographic_author_count   No. Observations:                69657\n",
      "Model:                                  GLM   Df Residuals:                    69655\n",
      "Model Family:              NegativeBinomial   Df Model:                            1\n",
      "Link Function:                          log   Scale:                          1.0000\n",
      "Method:                                IRLS   Log-Likelihood:                -6762.8\n",
      "Date:                      Tue, 28 Jan 2020   Deviance:                       2487.3\n",
      "Time:                              19:42:31   Pearson chi2:                 1.49e+05\n",
      "No. Iterations:                          11   Covariance Type:             nonrobust\n",
      "===================================================================================\n",
      "                      coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------\n",
      "demographic       -20.5080      0.095   -215.054      0.000     -20.695     -20.321\n",
      "primary_subject     1.1980      0.102     11.752      0.000       0.998       1.398\n",
      "===================================================================================\n"
     ]
    }
   ],
   "source": [
    "poisson_training_results = sm.GLM(df['demographic_author_count'], df[['demographic', 'primary_subject']], \n",
    "                                  family=sm.families.Poisson()).fit()\n",
    "df['BB_LAMBDA'] = poisson_training_results.mu\n",
    "df['AUX_OLS_DEP'] = df.apply(lambda x: ((x['demographic_author_count'] - \n",
    "                                                     x['BB_LAMBDA'])**2 - x['demographic_author_count']) / x['BB_LAMBDA'], axis=1)\n",
    "ols_expr = \"\"\"AUX_OLS_DEP ~ BB_LAMBDA - 1\"\"\"\n",
    "aux_olsr_results = smf.ols(ols_expr, df).fit()\n",
    "nb2_training_results = sm.GLM(df['demographic_author_count'], df[['demographic', 'primary_subject']], \n",
    "                              offset=df['word_count'],\n",
    "                              family=sm.families.NegativeBinomial(alpha=aux_olsr_results.params[0])).fit()\n",
    "print(nb2_training_results.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relational"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Generalized Linear Model Regression Results                    \n",
      "===================================================================================\n",
      "Dep. Variable:     relational_author_count   No. Observations:                69657\n",
      "Model:                                 GLM   Df Residuals:                    69655\n",
      "Model Family:             NegativeBinomial   Df Model:                            1\n",
      "Link Function:                         log   Scale:                          1.0000\n",
      "Method:                               IRLS   Log-Likelihood:                -5429.1\n",
      "Date:                     Tue, 28 Jan 2020   Deviance:                       2471.5\n",
      "Time:                             19:42:38   Pearson chi2:                 7.08e+04\n",
      "No. Iterations:                         12   Covariance Type:             nonrobust\n",
      "===================================================================================\n",
      "                      coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------\n",
      "relational        -30.6119      0.151   -203.011      0.000     -30.907     -30.316\n",
      "primary_subject     2.0474      0.094     21.831      0.000       1.864       2.231\n",
      "===================================================================================\n"
     ]
    }
   ],
   "source": [
    "poisson_training_results = sm.GLM(df['relational_author_count'], df[['relational', 'primary_subject']], \n",
    "                                  family=sm.families.Poisson()).fit()\n",
    "df['BB_LAMBDA'] = poisson_training_results.mu\n",
    "df['AUX_OLS_DEP'] = df.apply(lambda x: ((x['relational_author_count'] - \n",
    "                                                     x['BB_LAMBDA'])**2 - x['relational_author_count']) / x['BB_LAMBDA'], axis=1)\n",
    "ols_expr = \"\"\"AUX_OLS_DEP ~ BB_LAMBDA - 1\"\"\"\n",
    "aux_olsr_results = smf.ols(ols_expr, df).fit()\n",
    "nb2_training_results = sm.GLM(df['relational_author_count'], df[['relational', 'primary_subject']], \n",
    "                              offset=df['word_count'],\n",
    "                              family=sm.families.NegativeBinomial(alpha=aux_olsr_results.params[0])).fit()\n",
    "print(nb2_training_results.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec\n",
    "### Culture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Generalized Linear Model Regression Results                   \n",
      "=================================================================================\n",
      "Dep. Variable:     cultural_author_count   No. Observations:                69657\n",
      "Model:                               GLM   Df Residuals:                    69655\n",
      "Model Family:           NegativeBinomial   Df Model:                            1\n",
      "Link Function:                       log   Scale:                          1.0000\n",
      "Method:                             IRLS   Log-Likelihood:                -12193.\n",
      "Date:                   Tue, 28 Jan 2020   Deviance:                       10555.\n",
      "Time:                           19:42:47   Pearson chi2:                 4.22e+05\n",
      "No. Iterations:                       19   Covariance Type:             nonrobust\n",
      "===========================================================================================\n",
      "                              coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------------\n",
      "culture_word2vec_cosine   -16.8302      0.052   -324.922      0.000     -16.932     -16.729\n",
      "primary_subject             0.9137      0.058     15.658      0.000       0.799       1.028\n",
      "===========================================================================================\n"
     ]
    }
   ],
   "source": [
    "poisson_training_results = sm.GLM(df['cultural_author_count'], df[['culture_word2vec_cosine', 'primary_subject']], \n",
    "                                  family=sm.families.Poisson()).fit()\n",
    "df['BB_LAMBDA'] = poisson_training_results.mu\n",
    "df['AUX_OLS_DEP'] = df.apply(lambda x: ((x['cultural_author_count'] - \n",
    "                                                     x['BB_LAMBDA'])**2 - x['cultural_author_count']) / x['BB_LAMBDA'], axis=1)\n",
    "ols_expr = \"\"\"AUX_OLS_DEP ~ BB_LAMBDA - 1\"\"\"\n",
    "aux_olsr_results = smf.ols(ols_expr, df).fit()\n",
    "nb2_training_results = sm.GLM(df['cultural_author_count'], df[['culture_word2vec_cosine', 'primary_subject']], \n",
    "                              offset=df['word_count'],\n",
    "                              family=sm.families.NegativeBinomial(alpha=aux_olsr_results.params[0])).fit()\n",
    "print(nb2_training_results.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demographic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Generalized Linear Model Regression Results                     \n",
      "====================================================================================\n",
      "Dep. Variable:     demographic_author_count   No. Observations:                69657\n",
      "Model:                                  GLM   Df Residuals:                    69655\n",
      "Model Family:              NegativeBinomial   Df Model:                            1\n",
      "Link Function:                          log   Scale:                          1.0000\n",
      "Method:                                IRLS   Log-Likelihood:                -8273.3\n",
      "Date:                      Tue, 28 Jan 2020   Deviance:                       6075.9\n",
      "Time:                              19:43:12   Pearson chi2:                 5.62e+05\n",
      "No. Iterations:                          22   Covariance Type:             nonrobust\n",
      "===============================================================================================\n",
      "                                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------------------\n",
      "demographic_word2vec_cosine   -17.0011      0.068   -249.889      0.000     -17.134     -16.868\n",
      "primary_subject                 1.6022      0.081     19.772      0.000       1.443       1.761\n",
      "===============================================================================================\n"
     ]
    }
   ],
   "source": [
    "poisson_training_results = sm.GLM(df['demographic_author_count'], df[['demographic_word2vec_cosine', 'primary_subject']], \n",
    "                                  family=sm.families.Poisson()).fit()\n",
    "df['BB_LAMBDA'] = poisson_training_results.mu\n",
    "df['AUX_OLS_DEP'] = df.apply(lambda x: ((x['demographic_author_count'] - \n",
    "                                                     x['BB_LAMBDA'])**2 - x['demographic_author_count']) / x['BB_LAMBDA'], axis=1)\n",
    "ols_expr = \"\"\"AUX_OLS_DEP ~ BB_LAMBDA - 1\"\"\"\n",
    "aux_olsr_results = smf.ols(ols_expr, df).fit()\n",
    "nb2_training_results = sm.GLM(df['demographic_author_count'], df[['demographic_word2vec_cosine', 'primary_subject']], \n",
    "                              offset=df['word_count'],\n",
    "                              family=sm.families.NegativeBinomial(alpha=aux_olsr_results.params[0])).fit()\n",
    "print(nb2_training_results.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relational"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Generalized Linear Model Regression Results                    \n",
      "===================================================================================\n",
      "Dep. Variable:     relational_author_count   No. Observations:                69657\n",
      "Model:                                 GLM   Df Residuals:                    69655\n",
      "Model Family:             NegativeBinomial   Df Model:                            1\n",
      "Link Function:                         log   Scale:                          1.0000\n",
      "Method:                               IRLS   Log-Likelihood:                -7010.7\n",
      "Date:                     Tue, 28 Jan 2020   Deviance:                       6128.9\n",
      "Time:                             19:43:19   Pearson chi2:                 8.78e+05\n",
      "No. Iterations:                         21   Covariance Type:             nonrobust\n",
      "==============================================================================================\n",
      "                                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------------------\n",
      "relational_word2vec_cosine   -19.2595      0.071   -272.592      0.000     -19.398     -19.121\n",
      "primary_subject                3.7850      0.072     52.521      0.000       3.644       3.926\n",
      "==============================================================================================\n"
     ]
    }
   ],
   "source": [
    "poisson_training_results = sm.GLM(df['relational_author_count'], df[['relational_word2vec_cosine', 'primary_subject']], \n",
    "                                  family=sm.families.Poisson()).fit()\n",
    "df['BB_LAMBDA'] = poisson_training_results.mu\n",
    "df['AUX_OLS_DEP'] = df.apply(lambda x: ((x['relational_author_count'] - \n",
    "                                                     x['BB_LAMBDA'])**2 - x['relational_author_count']) / x['BB_LAMBDA'], axis=1)\n",
    "ols_expr = \"\"\"AUX_OLS_DEP ~ BB_LAMBDA - 1\"\"\"\n",
    "aux_olsr_results = smf.ols(ols_expr, df).fit()\n",
    "nb2_training_results = sm.GLM(df['relational_author_count'], df[['relational_word2vec_cosine', 'primary_subject']], \n",
    "                              offset=df['word_count'],\n",
    "                              family=sm.families.NegativeBinomial(alpha=aux_olsr_results.params[0])).fit()\n",
    "print(nb2_training_results.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2vec\n",
    "### Culture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Generalized Linear Model Regression Results                   \n",
      "=================================================================================\n",
      "Dep. Variable:     cultural_author_count   No. Observations:                69657\n",
      "Model:                               GLM   Df Residuals:                    69655\n",
      "Model Family:           NegativeBinomial   Df Model:                            1\n",
      "Link Function:                       log   Scale:                          1.0000\n",
      "Method:                             IRLS   Log-Likelihood:                -61915.\n",
      "Date:                   Tue, 28 Jan 2020   Deviance:                   1.1565e+05\n",
      "Time:                           19:43:23   Pearson chi2:                 3.89e+17\n",
      "No. Iterations:                       53   Covariance Type:             nonrobust\n",
      "==========================================================================================\n",
      "                             coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------------------\n",
      "culture_doc2vec_cosine  -232.1308      0.494   -470.293      0.000    -233.098    -231.163\n",
      "primary_subject           -4.3655      0.027   -161.292      0.000      -4.419      -4.312\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "poisson_training_results = sm.GLM(df['cultural_author_count'], df[['culture_doc2vec_cosine', 'primary_subject']], \n",
    "                                  family=sm.families.Poisson()).fit()\n",
    "df['BB_LAMBDA'] = poisson_training_results.mu\n",
    "df['AUX_OLS_DEP'] = df.apply(lambda x: ((x['cultural_author_count'] - \n",
    "                                                     x['BB_LAMBDA'])**2 - x['cultural_author_count']) / x['BB_LAMBDA'], axis=1)\n",
    "ols_expr = \"\"\"AUX_OLS_DEP ~ BB_LAMBDA - 1\"\"\"\n",
    "aux_olsr_results = smf.ols(ols_expr, df).fit()\n",
    "nb2_training_results = sm.GLM(df['cultural_author_count'], df[['culture_doc2vec_cosine', 'primary_subject']], \n",
    "                              offset=df['word_count'],\n",
    "                              family=sm.families.NegativeBinomial(alpha=aux_olsr_results.params[0])).fit()\n",
    "print(nb2_training_results.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demographic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Generalized Linear Model Regression Results                     \n",
      "====================================================================================\n",
      "Dep. Variable:     demographic_author_count   No. Observations:                69657\n",
      "Model:                                  GLM   Df Residuals:                    69655\n",
      "Model Family:              NegativeBinomial   Df Model:                            1\n",
      "Link Function:                          log   Scale:                          1.0000\n",
      "Method:                                IRLS   Log-Likelihood:                -51172.\n",
      "Date:                      Tue, 28 Jan 2020   Deviance:                       96808.\n",
      "Time:                              19:43:27   Pearson chi2:                 2.43e+17\n",
      "No. Iterations:                          59   Covariance Type:             nonrobust\n",
      "==============================================================================================\n",
      "                                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------------------\n",
      "demographic_doc2vec_cosine  -265.6306      0.615   -432.000      0.000    -266.836    -264.425\n",
      "primary_subject               -4.3216      0.030   -142.092      0.000      -4.381      -4.262\n",
      "==============================================================================================\n"
     ]
    }
   ],
   "source": [
    "poisson_training_results = sm.GLM(df['demographic_author_count'], df[['demographic_doc2vec_cosine', 'primary_subject']], \n",
    "                                  family=sm.families.Poisson()).fit()\n",
    "df['BB_LAMBDA'] = poisson_training_results.mu\n",
    "df['AUX_OLS_DEP'] = df.apply(lambda x: ((x['demographic_author_count'] - \n",
    "                                                     x['BB_LAMBDA'])**2 - x['demographic_author_count']) / x['BB_LAMBDA'], axis=1)\n",
    "ols_expr = \"\"\"AUX_OLS_DEP ~ BB_LAMBDA - 1\"\"\"\n",
    "aux_olsr_results = smf.ols(ols_expr, df).fit()\n",
    "nb2_training_results = sm.GLM(df['demographic_author_count'], df[['demographic_doc2vec_cosine', 'primary_subject']], \n",
    "                              offset=df['word_count'],\n",
    "                              family=sm.families.NegativeBinomial(alpha=aux_olsr_results.params[0])).fit()\n",
    "print(nb2_training_results.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relational"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Generalized Linear Model Regression Results                    \n",
      "===================================================================================\n",
      "Dep. Variable:     relational_author_count   No. Observations:                69657\n",
      "Model:                                 GLM   Df Residuals:                    69655\n",
      "Model Family:             NegativeBinomial   Df Model:                            1\n",
      "Link Function:                         log   Scale:                          1.0000\n",
      "Method:                               IRLS   Log-Likelihood:                -58562.\n",
      "Date:                     Tue, 28 Jan 2020   Deviance:                   1.1310e+05\n",
      "Time:                             19:43:32   Pearson chi2:                 2.24e+18\n",
      "No. Iterations:                         59   Covariance Type:             nonrobust\n",
      "=============================================================================================\n",
      "                                coef    std err          z      P>|z|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------------------\n",
      "relational_doc2vec_cosine  -392.4016      0.876   -448.046      0.000    -394.118    -390.685\n",
      "primary_subject              -4.6005      0.030   -153.654      0.000      -4.659      -4.542\n",
      "=============================================================================================\n"
     ]
    }
   ],
   "source": [
    "poisson_training_results = sm.GLM(df['relational_author_count'], df[['relational_doc2vec_cosine', 'primary_subject']], \n",
    "                                  family=sm.families.Poisson()).fit()\n",
    "df['BB_LAMBDA'] = poisson_training_results.mu\n",
    "df['AUX_OLS_DEP'] = df.apply(lambda x: ((x['relational_author_count'] - \n",
    "                                                     x['BB_LAMBDA'])**2 - x['relational_author_count']) / x['BB_LAMBDA'], axis=1)\n",
    "ols_expr = \"\"\"AUX_OLS_DEP ~ BB_LAMBDA - 1\"\"\"\n",
    "aux_olsr_results = smf.ols(ols_expr, df).fit()\n",
    "nb2_training_results = sm.GLM(df['relational_author_count'], df[['relational_doc2vec_cosine', 'primary_subject']], \n",
    "                              offset=df['word_count'],\n",
    "                              family=sm.families.NegativeBinomial(alpha=aux_olsr_results.params[0])).fit()\n",
    "print(nb2_training_results.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe\n",
    "### Culture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Generalized Linear Model Regression Results                   \n",
      "=================================================================================\n",
      "Dep. Variable:     cultural_author_count   No. Observations:                69657\n",
      "Model:                               GLM   Df Residuals:                    69655\n",
      "Model Family:           NegativeBinomial   Df Model:                            1\n",
      "Link Function:                       log   Scale:                          1.0000\n",
      "Method:                             IRLS   Log-Likelihood:                -11353.\n",
      "Date:                   Tue, 28 Jan 2020   Deviance:                       8910.1\n",
      "Time:                           19:43:35   Pearson chi2:                 3.50e+05\n",
      "No. Iterations:                       17   Covariance Type:             nonrobust\n",
      "========================================================================================\n",
      "                           coef    std err          z      P>|z|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------------\n",
      "culture_glove_cosine   -19.2453      0.059   -324.489      0.000     -19.362     -19.129\n",
      "primary_subject          0.8772      0.059     14.809      0.000       0.761       0.993\n",
      "========================================================================================\n"
     ]
    }
   ],
   "source": [
    "poisson_training_results = sm.GLM(df['cultural_author_count'], df[['culture_glove_cosine', 'primary_subject']], \n",
    "                                  family=sm.families.Poisson()).fit()\n",
    "df['BB_LAMBDA'] = poisson_training_results.mu\n",
    "df['AUX_OLS_DEP'] = df.apply(lambda x: ((x['cultural_author_count'] - \n",
    "                                                     x['BB_LAMBDA'])**2 - x['cultural_author_count']) / x['BB_LAMBDA'], axis=1)\n",
    "ols_expr = \"\"\"AUX_OLS_DEP ~ BB_LAMBDA - 1\"\"\"\n",
    "aux_olsr_results = smf.ols(ols_expr, df).fit()\n",
    "nb2_training_results = sm.GLM(df['cultural_author_count'], df[['culture_glove_cosine', 'primary_subject']], \n",
    "                              offset=df['word_count'],\n",
    "                              family=sm.families.NegativeBinomial(alpha=aux_olsr_results.params[0])).fit()\n",
    "print(nb2_training_results.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demographic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Generalized Linear Model Regression Results                     \n",
      "====================================================================================\n",
      "Dep. Variable:     demographic_author_count   No. Observations:                69657\n",
      "Model:                                  GLM   Df Residuals:                    69655\n",
      "Model Family:              NegativeBinomial   Df Model:                            1\n",
      "Link Function:                          log   Scale:                          1.0000\n",
      "Method:                                IRLS   Log-Likelihood:                -7601.3\n",
      "Date:                      Tue, 28 Jan 2020   Deviance:                       4989.2\n",
      "Time:                              19:43:39   Pearson chi2:                 3.49e+05\n",
      "No. Iterations:                          17   Covariance Type:             nonrobust\n",
      "============================================================================================\n",
      "                               coef    std err          z      P>|z|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------------------\n",
      "demographic_glove_cosine   -17.3507      0.066   -263.220      0.000     -17.480     -17.222\n",
      "primary_subject              1.4961      0.079     18.984      0.000       1.342       1.651\n",
      "============================================================================================\n"
     ]
    }
   ],
   "source": [
    "poisson_training_results = sm.GLM(df['demographic_author_count'], df[['demographic_glove_cosine', 'primary_subject']], \n",
    "                                  family=sm.families.Poisson()).fit()\n",
    "df['BB_LAMBDA'] = poisson_training_results.mu\n",
    "df['AUX_OLS_DEP'] = df.apply(lambda x: ((x['demographic_author_count'] - \n",
    "                                                     x['BB_LAMBDA'])**2 - x['demographic_author_count']) / x['BB_LAMBDA'], axis=1)\n",
    "ols_expr = \"\"\"AUX_OLS_DEP ~ BB_LAMBDA - 1\"\"\"\n",
    "aux_olsr_results = smf.ols(ols_expr, df).fit()\n",
    "nb2_training_results = sm.GLM(df['demographic_author_count'], df[['demographic_glove_cosine', 'primary_subject']], \n",
    "                              offset=df['word_count'],\n",
    "                              family=sm.families.NegativeBinomial(alpha=aux_olsr_results.params[0])).fit()\n",
    "print(nb2_training_results.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relational"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Generalized Linear Model Regression Results                    \n",
      "===================================================================================\n",
      "Dep. Variable:     relational_author_count   No. Observations:                69657\n",
      "Model:                                 GLM   Df Residuals:                    69655\n",
      "Model Family:             NegativeBinomial   Df Model:                            1\n",
      "Link Function:                         log   Scale:                          1.0000\n",
      "Method:                               IRLS   Log-Likelihood:                -6469.4\n",
      "Date:                     Tue, 28 Jan 2020   Deviance:                       5528.4\n",
      "Time:                             19:43:42   Pearson chi2:                 3.25e+05\n",
      "No. Iterations:                         16   Covariance Type:             nonrobust\n",
      "===========================================================================================\n",
      "                              coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------------\n",
      "relational_glove_cosine   -17.9699      0.068   -263.439      0.000     -18.104     -17.836\n",
      "primary_subject             2.9820      0.069     42.955      0.000       2.846       3.118\n",
      "===========================================================================================\n"
     ]
    }
   ],
   "source": [
    "poisson_training_results = sm.GLM(df['relational_author_count'], df[['relational_glove_cosine', 'primary_subject']], \n",
    "                                  family=sm.families.Poisson()).fit()\n",
    "df['BB_LAMBDA'] = poisson_training_results.mu\n",
    "df['AUX_OLS_DEP'] = df.apply(lambda x: ((x['relational_author_count'] - \n",
    "                                                     x['BB_LAMBDA'])**2 - x['relational_author_count']) / x['BB_LAMBDA'], axis=1)\n",
    "ols_expr = \"\"\"AUX_OLS_DEP ~ BB_LAMBDA - 1\"\"\"\n",
    "aux_olsr_results = smf.ols(ols_expr, df).fit()\n",
    "nb2_training_results = sm.GLM(df['relational_author_count'], df[['relational_glove_cosine', 'primary_subject']], \n",
    "                              offset=df['word_count'],\n",
    "                              family=sm.families.NegativeBinomial(alpha=aux_olsr_results.params[0])).fit()\n",
    "print(nb2_training_results.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ngram\n",
    "### culture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Generalized Linear Model Regression Results                   \n",
      "=================================================================================\n",
      "Dep. Variable:     cultural_author_count   No. Observations:                69657\n",
      "Model:                               GLM   Df Residuals:                    69655\n",
      "Model Family:           NegativeBinomial   Df Model:                            1\n",
      "Link Function:                       log   Scale:                          1.0000\n",
      "Method:                             IRLS   Log-Likelihood:            -2.5890e+05\n",
      "Date:                   Tue, 28 Jan 2020   Deviance:                   5.1119e+05\n",
      "Time:                           19:48:13   Pearson chi2:                 9.81e+18\n",
      "No. Iterations:                       83   Covariance Type:             nonrobust\n",
      "=======================================================================================\n",
      "                          coef    std err          z      P>|z|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------------\n",
      "culture_ngram_count    -0.1111      0.000   -480.132      0.000      -0.112      -0.111\n",
      "primary_subject        -8.4945      0.021   -400.765      0.000      -8.536      -8.453\n",
      "=======================================================================================\n"
     ]
    }
   ],
   "source": [
    "poisson_training_results = sm.GLM(df['cultural_author_count'], df[['culture_ngram_count', 'primary_subject']], \n",
    "                                  family=sm.families.Poisson()).fit()\n",
    "df['BB_LAMBDA'] = poisson_training_results.mu\n",
    "df['AUX_OLS_DEP'] = df.apply(lambda x: ((x['cultural_author_count'] - \n",
    "                                                     x['BB_LAMBDA'])**2 - x['cultural_author_count']) / x['BB_LAMBDA'], axis=1)\n",
    "ols_expr = \"\"\"AUX_OLS_DEP ~ BB_LAMBDA - 1\"\"\"\n",
    "aux_olsr_results = smf.ols(ols_expr, df).fit()\n",
    "nb2_training_results = sm.GLM(df['cultural_author_count'], df[['culture_ngram_count', 'primary_subject']], \n",
    "                              offset=df['word_count'],\n",
    "                              family=sm.families.NegativeBinomial(alpha=aux_olsr_results.params[0])).fit()\n",
    "print(nb2_training_results.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demographic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Generalized Linear Model Regression Results                     \n",
      "====================================================================================\n",
      "Dep. Variable:     demographic_author_count   No. Observations:                69657\n",
      "Model:                                  GLM   Df Residuals:                    69655\n",
      "Model Family:              NegativeBinomial   Df Model:                            1\n",
      "Link Function:                          log   Scale:                          1.0000\n",
      "Method:                                IRLS   Log-Likelihood:            -2.8046e+05\n",
      "Date:                      Tue, 28 Jan 2020   Deviance:                   5.5666e+05\n",
      "Time:                              19:48:18   Pearson chi2:                 2.48e+19\n",
      "No. Iterations:                         100   Covariance Type:             nonrobust\n",
      "===========================================================================================\n",
      "                              coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------------\n",
      "demographic_ngram_count    -0.1017      0.000   -405.853      0.000      -0.102      -0.101\n",
      "primary_subject            -9.3137      0.025   -379.908      0.000      -9.362      -9.266\n",
      "===========================================================================================\n"
     ]
    }
   ],
   "source": [
    "poisson_training_results = sm.GLM(df['demographic_author_count'], df[['demographic_ngram_count', 'primary_subject']], \n",
    "                                  family=sm.families.Poisson()).fit()\n",
    "df['BB_LAMBDA'] = poisson_training_results.mu\n",
    "df['AUX_OLS_DEP'] = df.apply(lambda x: ((x['demographic_author_count'] - \n",
    "                                                     x['BB_LAMBDA'])**2 - x['demographic_author_count']) / x['BB_LAMBDA'], axis=1)\n",
    "ols_expr = \"\"\"AUX_OLS_DEP ~ BB_LAMBDA - 1\"\"\"\n",
    "aux_olsr_results = smf.ols(ols_expr, df).fit()\n",
    "nb2_training_results = sm.GLM(df['demographic_author_count'], df[['demographic_ngram_count', 'primary_subject']], \n",
    "                              offset=df['word_count'],\n",
    "                              family=sm.families.NegativeBinomial(alpha=aux_olsr_results.params[0])).fit()\n",
    "print(nb2_training_results.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relational"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Generalized Linear Model Regression Results                    \n",
      "===================================================================================\n",
      "Dep. Variable:     relational_author_count   No. Observations:                69657\n",
      "Model:                                 GLM   Df Residuals:                    69655\n",
      "Model Family:             NegativeBinomial   Df Model:                            1\n",
      "Link Function:                         log   Scale:                          1.0000\n",
      "Method:                               IRLS   Log-Likelihood:            -1.9552e+05\n",
      "Date:                     Tue, 28 Jan 2020   Deviance:                   3.8754e+05\n",
      "Time:                             19:48:24   Pearson chi2:                 3.01e+19\n",
      "No. Iterations:                         68   Covariance Type:             nonrobust\n",
      "==========================================================================================\n",
      "                             coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------------------\n",
      "relational_ngram_count    -0.2025      0.000   -750.829      0.000      -0.203      -0.202\n",
      "primary_subject           -4.9599      0.026   -189.521      0.000      -5.011      -4.909\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "poisson_training_results = sm.GLM(df['relational_author_count'], df[['relational_ngram_count', 'primary_subject']], \n",
    "                                  family=sm.families.Poisson()).fit()\n",
    "df['BB_LAMBDA'] = poisson_training_results.mu\n",
    "df['AUX_OLS_DEP'] = df.apply(lambda x: ((x['relational_author_count'] - \n",
    "                                                     x['BB_LAMBDA'])**2 - x['relational_author_count']) / x['BB_LAMBDA'], axis=1)\n",
    "ols_expr = \"\"\"AUX_OLS_DEP ~ BB_LAMBDA - 1\"\"\"\n",
    "aux_olsr_results = smf.ols(ols_expr, df).fit()\n",
    "nb2_training_results = sm.GLM(df['relational_author_count'], df[['relational_ngram_count', 'primary_subject']], \n",
    "                              offset=df['word_count'],\n",
    "                              family=sm.families.NegativeBinomial(alpha=aux_olsr_results.params[0])).fit()\n",
    "print(nb2_training_results.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>mean_se</th>\n",
       "      <th>mean_ci_lower</th>\n",
       "      <th>mean_ci_upper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.832684</td>\n",
       "      <td>0.000203</td>\n",
       "      <td>0.832286</td>\n",
       "      <td>0.833082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.831429</td>\n",
       "      <td>0.000204</td>\n",
       "      <td>0.831029</td>\n",
       "      <td>0.831830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.834250</td>\n",
       "      <td>0.000201</td>\n",
       "      <td>0.833855</td>\n",
       "      <td>0.834644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.831088</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>0.830686</td>\n",
       "      <td>0.831489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.005793</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.005503</td>\n",
       "      <td>0.006097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.830027</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>0.829623</td>\n",
       "      <td>0.830431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.835386</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.834994</td>\n",
       "      <td>0.835778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.828172</td>\n",
       "      <td>0.000208</td>\n",
       "      <td>0.827765</td>\n",
       "      <td>0.828580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.824855</td>\n",
       "      <td>0.000212</td>\n",
       "      <td>0.824441</td>\n",
       "      <td>0.825270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.005871</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.005578</td>\n",
       "      <td>0.006180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.005845</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.005553</td>\n",
       "      <td>0.006152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.832524</td>\n",
       "      <td>0.000203</td>\n",
       "      <td>0.832126</td>\n",
       "      <td>0.832922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.830407</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>0.830004</td>\n",
       "      <td>0.830810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.826266</td>\n",
       "      <td>0.000210</td>\n",
       "      <td>0.825855</td>\n",
       "      <td>0.826678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.828319</td>\n",
       "      <td>0.000208</td>\n",
       "      <td>0.827911</td>\n",
       "      <td>0.828726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.828009</td>\n",
       "      <td>0.000208</td>\n",
       "      <td>0.827601</td>\n",
       "      <td>0.828417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.830293</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>0.829890</td>\n",
       "      <td>0.830697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.005855</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.005563</td>\n",
       "      <td>0.006163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.833265</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>0.832869</td>\n",
       "      <td>0.833662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.829564</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>0.829159</td>\n",
       "      <td>0.829968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.005822</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>0.005531</td>\n",
       "      <td>0.006128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.829320</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.828915</td>\n",
       "      <td>0.829725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.830724</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>0.830321</td>\n",
       "      <td>0.831126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.830799</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>0.830397</td>\n",
       "      <td>0.831201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.832578</td>\n",
       "      <td>0.000203</td>\n",
       "      <td>0.832180</td>\n",
       "      <td>0.832977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.827678</td>\n",
       "      <td>0.000208</td>\n",
       "      <td>0.827269</td>\n",
       "      <td>0.828087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.834948</td>\n",
       "      <td>0.000201</td>\n",
       "      <td>0.834555</td>\n",
       "      <td>0.835341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.831587</td>\n",
       "      <td>0.000204</td>\n",
       "      <td>0.831187</td>\n",
       "      <td>0.831988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.826521</td>\n",
       "      <td>0.000210</td>\n",
       "      <td>0.826110</td>\n",
       "      <td>0.826932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.005869</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.005576</td>\n",
       "      <td>0.006178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113277</th>\n",
       "      <td>0.831076</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>0.830675</td>\n",
       "      <td>0.831478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113279</th>\n",
       "      <td>0.833750</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>0.833354</td>\n",
       "      <td>0.834145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113281</th>\n",
       "      <td>0.005795</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.005506</td>\n",
       "      <td>0.006100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113282</th>\n",
       "      <td>0.830672</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>0.830270</td>\n",
       "      <td>0.831075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113283</th>\n",
       "      <td>0.005828</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>0.005537</td>\n",
       "      <td>0.006134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113287</th>\n",
       "      <td>0.829010</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.828605</td>\n",
       "      <td>0.829416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113288</th>\n",
       "      <td>0.005812</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>0.005522</td>\n",
       "      <td>0.006118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113291</th>\n",
       "      <td>0.828963</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.828557</td>\n",
       "      <td>0.829369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113294</th>\n",
       "      <td>0.828235</td>\n",
       "      <td>0.000208</td>\n",
       "      <td>0.827828</td>\n",
       "      <td>0.828643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113295</th>\n",
       "      <td>0.830167</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>0.829763</td>\n",
       "      <td>0.830570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113299</th>\n",
       "      <td>0.005827</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>0.005536</td>\n",
       "      <td>0.006133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113303</th>\n",
       "      <td>0.005845</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.005553</td>\n",
       "      <td>0.006152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113305</th>\n",
       "      <td>0.005902</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.005607</td>\n",
       "      <td>0.006212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113306</th>\n",
       "      <td>0.005846</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.005554</td>\n",
       "      <td>0.006154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113308</th>\n",
       "      <td>0.829527</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>0.829123</td>\n",
       "      <td>0.829932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113310</th>\n",
       "      <td>0.833845</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>0.833449</td>\n",
       "      <td>0.834240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113313</th>\n",
       "      <td>0.005829</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>0.005538</td>\n",
       "      <td>0.006136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113315</th>\n",
       "      <td>0.824236</td>\n",
       "      <td>0.000212</td>\n",
       "      <td>0.823820</td>\n",
       "      <td>0.824652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113318</th>\n",
       "      <td>0.826945</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>0.826535</td>\n",
       "      <td>0.827355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113323</th>\n",
       "      <td>0.830268</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>0.829865</td>\n",
       "      <td>0.830671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113324</th>\n",
       "      <td>0.832241</td>\n",
       "      <td>0.000204</td>\n",
       "      <td>0.831842</td>\n",
       "      <td>0.832640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113326</th>\n",
       "      <td>0.830250</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>0.829847</td>\n",
       "      <td>0.830654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113332</th>\n",
       "      <td>0.005825</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>0.005534</td>\n",
       "      <td>0.006131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113333</th>\n",
       "      <td>0.839135</td>\n",
       "      <td>0.000196</td>\n",
       "      <td>0.838750</td>\n",
       "      <td>0.839519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113335</th>\n",
       "      <td>0.831626</td>\n",
       "      <td>0.000204</td>\n",
       "      <td>0.831226</td>\n",
       "      <td>0.832026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113336</th>\n",
       "      <td>0.005846</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.005554</td>\n",
       "      <td>0.006153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113339</th>\n",
       "      <td>0.831824</td>\n",
       "      <td>0.000204</td>\n",
       "      <td>0.831424</td>\n",
       "      <td>0.832224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113340</th>\n",
       "      <td>0.825400</td>\n",
       "      <td>0.000211</td>\n",
       "      <td>0.824987</td>\n",
       "      <td>0.825814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113341</th>\n",
       "      <td>0.005865</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.005572</td>\n",
       "      <td>0.006173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113342</th>\n",
       "      <td>0.826323</td>\n",
       "      <td>0.000210</td>\n",
       "      <td>0.825912</td>\n",
       "      <td>0.826734</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>55841 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            mean   mean_se  mean_ci_lower  mean_ci_upper\n",
       "0       0.832684  0.000203       0.832286       0.833082\n",
       "1       0.831429  0.000204       0.831029       0.831830\n",
       "3       0.834250  0.000201       0.833855       0.834644\n",
       "6       0.831088  0.000205       0.830686       0.831489\n",
       "10      0.005793  0.000151       0.005503       0.006097\n",
       "11      0.830027  0.000206       0.829623       0.830431\n",
       "12      0.835386  0.000200       0.834994       0.835778\n",
       "14      0.828172  0.000208       0.827765       0.828580\n",
       "16      0.824855  0.000212       0.824441       0.825270\n",
       "17      0.005871  0.000153       0.005578       0.006180\n",
       "19      0.005845  0.000153       0.005553       0.006152\n",
       "20      0.832524  0.000203       0.832126       0.832922\n",
       "24      0.830407  0.000206       0.830004       0.830810\n",
       "25      0.826266  0.000210       0.825855       0.826678\n",
       "26      0.828319  0.000208       0.827911       0.828726\n",
       "27      0.828009  0.000208       0.827601       0.828417\n",
       "29      0.830293  0.000206       0.829890       0.830697\n",
       "31      0.005855  0.000153       0.005563       0.006163\n",
       "36      0.833265  0.000202       0.832869       0.833662\n",
       "37      0.829564  0.000206       0.829159       0.829968\n",
       "41      0.005822  0.000152       0.005531       0.006128\n",
       "42      0.829320  0.000207       0.828915       0.829725\n",
       "43      0.830724  0.000205       0.830321       0.831126\n",
       "45      0.830799  0.000205       0.830397       0.831201\n",
       "46      0.832578  0.000203       0.832180       0.832977\n",
       "49      0.827678  0.000208       0.827269       0.828087\n",
       "52      0.834948  0.000201       0.834555       0.835341\n",
       "53      0.831587  0.000204       0.831187       0.831988\n",
       "56      0.826521  0.000210       0.826110       0.826932\n",
       "58      0.005869  0.000153       0.005576       0.006178\n",
       "...          ...       ...            ...            ...\n",
       "113277  0.831076  0.000205       0.830675       0.831478\n",
       "113279  0.833750  0.000202       0.833354       0.834145\n",
       "113281  0.005795  0.000151       0.005506       0.006100\n",
       "113282  0.830672  0.000205       0.830270       0.831075\n",
       "113283  0.005828  0.000152       0.005537       0.006134\n",
       "113287  0.829010  0.000207       0.828605       0.829416\n",
       "113288  0.005812  0.000152       0.005522       0.006118\n",
       "113291  0.828963  0.000207       0.828557       0.829369\n",
       "113294  0.828235  0.000208       0.827828       0.828643\n",
       "113295  0.830167  0.000206       0.829763       0.830570\n",
       "113299  0.005827  0.000152       0.005536       0.006133\n",
       "113303  0.005845  0.000153       0.005553       0.006152\n",
       "113305  0.005902  0.000154       0.005607       0.006212\n",
       "113306  0.005846  0.000153       0.005554       0.006154\n",
       "113308  0.829527  0.000206       0.829123       0.829932\n",
       "113310  0.833845  0.000202       0.833449       0.834240\n",
       "113313  0.005829  0.000152       0.005538       0.006136\n",
       "113315  0.824236  0.000212       0.823820       0.824652\n",
       "113318  0.826945  0.000209       0.826535       0.827355\n",
       "113323  0.830268  0.000206       0.829865       0.830671\n",
       "113324  0.832241  0.000204       0.831842       0.832640\n",
       "113326  0.830250  0.000206       0.829847       0.830654\n",
       "113332  0.005825  0.000152       0.005534       0.006131\n",
       "113333  0.839135  0.000196       0.838750       0.839519\n",
       "113335  0.831626  0.000204       0.831226       0.832026\n",
       "113336  0.005846  0.000153       0.005554       0.006153\n",
       "113339  0.831824  0.000204       0.831424       0.832224\n",
       "113340  0.825400  0.000211       0.824987       0.825814\n",
       "113341  0.005865  0.000153       0.005572       0.006173\n",
       "113342  0.826323  0.000210       0.825912       0.826734\n",
       "\n",
       "[55841 rows x 4 columns]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb2_training_results.get_prediction(df_train[['culture', 'primary_subject']]).summary_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-111-0da0cae44832>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model_results' is not defined"
     ]
    }
   ],
   "source": [
    "model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expr = \"\"\"culture_ngram_count.1 ~ culture\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The first guess on the deviance function returned a nan.  This could be a boundary  problem and should be reported.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-8b1eed106603>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m poisson_training_results = sm.GLM(df['culture_ngram_count.1'], df[['culture']], \n\u001b[0;32m----> 2\u001b[0;31m                                   family=sm.families.Poisson()).fit()\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/statsmodels/genmod/generalized_linear_model.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, start_params, maxiter, method, tol, scale, cov_type, cov_kwds, use_t, full_output, disp, max_start_irls, **kwargs)\u001b[0m\n\u001b[1;32m   1010\u001b[0m             return self._fit_irls(start_params=start_params, maxiter=maxiter,\n\u001b[1;32m   1011\u001b[0m                                   \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcov_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcov_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1012\u001b[0;31m                                   cov_kwds=cov_kwds, use_t=use_t, **kwargs)\n\u001b[0m\u001b[1;32m   1013\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optim_hessian\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'optim_hessian'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/statsmodels/genmod/generalized_linear_model.py\u001b[0m in \u001b[0;36m_fit_irls\u001b[0;34m(self, start_params, maxiter, tol, scale, cov_type, cov_kwds, use_t, **kwargs)\u001b[0m\n\u001b[1;32m   1107\u001b[0m                                    self.freq_weights, self.scale)\n\u001b[1;32m   1108\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1109\u001b[0;31m             raise ValueError(\"The first guess on the deviance function \"\n\u001b[0m\u001b[1;32m   1110\u001b[0m                              \u001b[0;34m\"returned a nan.  This could be a boundary \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m                              \" problem and should be reported.\")\n",
      "\u001b[0;31mValueError\u001b[0m: The first guess on the deviance function returned a nan.  This could be a boundary  problem and should be reported."
     ]
    }
   ],
   "source": [
    "poisson_training_results = sm.GLM(df['culture_ngram_count.1'], df[['culture']], \n",
    "                                  family=sm.families.Poisson()).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['BB_LAMBDA'] = poisson_training_results.mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['AUX_OLS_DEP'] = df_train.apply(lambda x: \n",
    "                                         ((x['CITATION_COUNT'] - x['BB_LAMBDA'])**2 - \n",
    "                                          x['CITATION_COUNT']) / x['BB_LAMBDA'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_expr = \"\"\"AUX_OLS_DEP ~ BB_LAMBDA - 1\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_olsr_results = smf.ols(ols_expr, df).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(aux_olsr_results.params)\n",
    "print(aux_olsr_results.tvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb2_training_results = sm.GLM(df['CITATION_COUNT'], df[['COSINE_1', 'COSINE_2', 'COSINE_3']],\n",
    "                              family=sm.families.NegativeBinomial(alpha=aux_olsr_results.params[0])).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb2_training_results.mean"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
